{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ipynb_util_tars.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=thesis\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA3 8B - unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers.models.llama.modeling_llama import LlamaForSequenceClassification, LlamaDecoderLayer, LlamaConfig, LlamaRMSNorm, LlamaPreTrainedModel, LlamaModel, LLAMA_INPUTS_DOCSTRING, add_start_docstrings_to_model_forward, SequenceClassifierOutputWithPast, BaseModelOutputWithPast, BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from typing import Optional, List, Union, Tuple\n",
    "\n",
    "\n",
    "class UnmaskingLlamaModel(LlamaModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        if self.gradient_checkpointing and self.training and use_cache:\n",
    "            logger.warning_once(\n",
    "                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n",
    "            )\n",
    "            use_cache = False\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        return_legacy_cache = False\n",
    "        if use_cache and not isinstance(past_key_values, Cache):  # kept for BC (non `Cache` `past_key_values` inputs)\n",
    "            return_legacy_cache = True\n",
    "            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = self._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "        if causal_mask is not None:\n",
    "            #print(\"b4\", input_ids.shape, causal_mask.shape, causal_mask)\n",
    "            # Assuming causal_mask is a tensor with shape (batch_size, 1, seq_length, hidden_size)\n",
    "            causal_mask_last_row = causal_mask[:, :, -1, :].unsqueeze(2)\n",
    "            causal_mask = causal_mask_last_row.expand_as(causal_mask)\n",
    "            # causal_mask = torch.zeros_like(causal_mask, device=inputs_embeds.device)\n",
    "\n",
    "            #print(\"after\", causal_mask.shape, causal_mask)\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"kek it's none\", causal_mask, input_ids)\n",
    "\n",
    "        # embed positions\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    causal_mask,\n",
    "                    position_ids,\n",
    "                    past_key_values,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                    cache_position,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=causal_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if return_legacy_cache:\n",
    "            next_cache = next_cache.to_legacy_cache()\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "\n",
    "class UnmaskingLlamaForSequenceClassification(LlamaForSequenceClassification):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.model = UnmaskingLlamaModel(config)\n",
    "        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ZO_UP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select dataset\n",
    "from enum import Enum\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    \"\"\"Enum for the dataset type.\"\"\"\n",
    "\n",
    "    \"\"\"Zora + OSDG upsampled dataset\"\"\"\n",
    "    ZO_UP = \"zo_up\"\n",
    "    \"\"\"SwissText Shared Task 1 dataset (Zurich NLP)\"\"\"\n",
    "    SWISSTEXT_SHARED_TASK1 = \"swisstext_shared_task1\"\n",
    "    \"\"\"Toy dataset for testing\"\"\"\n",
    "    TOY = \"toy_data\"\n",
    "\n",
    "\n",
    "DATASET_TYPE = DatasetType.ZO_UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SDG': ClassLabel(names=['1', '10', '11', '12', '13', '14', '15', '16', '17', '2', '3', '4', '5', '6', '7', '8', '9'], id=None), 'ABSTRACT': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'sdg_desc_short': Value(dtype='string', id=None), 'sdg_desc_long': Value(dtype='string', id=None)}\n",
      "Example instance:\t {'SDG': 16, 'ABSTRACT': 'The first attempts to modernize simply replaced the single huge engine with a huge electric motor, changing little. The drive-shafts were replaced by wires, the huge steam engine by dozens of small motors. Factories spread out, there was natural light, and room to use ceiling-slung cranes. Workers had responsibility for their own machines, they needed better training and better pay. The electric motor was a wonderful invention, once we changed all the everyday details that surrounded it.', 'id': None, 'sdg_desc_short': None, 'sdg_desc_long': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Features, Value, ClassLabel, Dataset, DatasetDict\n",
    "import pickle\n",
    "\n",
    "# load the dataset\n",
    "# note: if you don't have the data in the folder, use the download-data.sh script\n",
    "\n",
    "match DATASET_TYPE:\n",
    "    case DatasetType.TOY:\n",
    "        dataset = DatasetDict({\n",
    "            \"train\": Dataset.from_list([{\"abstract\": \"hello world\", \"sdg\": 1}, {\"abstract\": \"Is this about clean energy?\", \"sdg\": 7}]),\n",
    "            \"test\": Dataset.from_list([{\"abstract\": \"kek what is this\", \"sdg\": 16}])\n",
    "        })\n",
    "        dataset = dataset.rename_columns({\"sdg\": \"SDG\", \"abstract\": \"ABSTRACT\"}).class_encode_column(\"SDG\")\n",
    "\n",
    "    case DatasetType.ZO_UP:\n",
    "        # dont need to use manual features as class_encode_column will create ClassLabel\n",
    "        # careful: watch out for the order of the ClassLabel as it doesn't map directly to the SDG class. need use mapping functions (id2label, label2id)\n",
    "        # sdgs = [str(i) for i in range(1, 18)] + [\"non-relevant\"]\n",
    "        # features = Features({\"sdg\": ClassLabel(num_classes=len(sdgs), names=sdgs), \"abstract\": Value(\"string\")})\n",
    "\n",
    "        dataset = load_dataset(\"csv\", data_files=str(DATA_DIR_PATH / \"zo_up.csv\"))\n",
    "        dataset = dataset.rename_columns({\"sdg\": \"SDG\", \"abstract\": \"ABSTRACT\"}).class_encode_column(\"SDG\")\n",
    "        dataset = dataset[\"train\"].train_test_split(test_size=0.3, stratify_by_column=\"SDG\", seed=SEED)\n",
    "    case DatasetType.SWISSTEXT_SHARED_TASK1:\n",
    "        dataset = load_dataset(\"json\", data_files=str(DATA_DIR_PATH / \"swisstext-2024-sharedtask\" / \"task1-train.jsonl\"))\n",
    "        dataset = dataset[\"train\"].train_test_split(test_size=0.3, seed=SEED)\n",
    "\n",
    "print(dataset[\"train\"].features)\n",
    "example = dataset[\"train\"][0]\n",
    "print(\"Example instance:\\t\", example)\n",
    "\n",
    "\n",
    "# Label encodings / mappings\n",
    "labels = set(dataset[\"train\"][\"SDG\"])\n",
    "id2label = {i: dataset[\"train\"].features[\"SDG\"].int2str(i) for i in range(len(labels))}\n",
    "label2id = {dataset[\"train\"].features[\"SDG\"].int2str(i): i for i in range(len(labels))}\n",
    "\n",
    "# save the encodings to a file for later use\n",
    "ENCODING_DIR = BASE_DIR_PATH / \"encodings\" / DATASET_TYPE.value\n",
    "# create the directory if it doesn't exist\n",
    "ENCODING_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(ENCODING_DIR / \"id2label.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id2label, f)\n",
    "\n",
    "with open(ENCODING_DIR / \"label2id.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label2id, f)\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <enum 'DatasetType'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <enum 'DatasetType'>: __main__.DatasetType has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# whether the text should be lowered or not\n",
    "SHOULD_LOWER = False\n",
    "\n",
    "# base model\n",
    "HF_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# final model\n",
    "MODEL_NAME = f\"{HF_MODEL_NAME}-ft-{DATASET_TYPE.value}\" + (\"-lower\" if SHOULD_LOWER else \"\") + \"-unmasked\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# print(tokenizer.pad_token, tokenizer.eos_token)\n",
    "\n",
    "\n",
    "def preprocess_data(instances):\n",
    "    match DATASET_TYPE:\n",
    "        case DatasetType.SWISSTEXT_SHARED_TASK1:\n",
    "            # take a batch of titles and abstracts and concat them\n",
    "            titles = instances[\"TITLE\"]\n",
    "            abstracts = instances[\"ABSTRACT\"]\n",
    "            texts = [f\"{title} {abstract}\" for title, abstract in zip(titles, abstracts)]\n",
    "        case DatasetType.ZO_UP | DatasetType.TOY:\n",
    "            texts = instances[\"ABSTRACT\"]\n",
    "\n",
    "\n",
    "    if SHOULD_LOWER:\n",
    "        texts = [text.lower() for text in texts]\n",
    "\n",
    "    # encode\n",
    "    encoding = tokenizer(texts, padding=\"longest\", truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "    # add labels\n",
    "    encoding[\"label\"] = torch.tensor([label for label in instances[\"SDG\"]])\n",
    "\n",
    "    return encoding\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668e4a5ee3834a02b83340032bdee5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of UnmaskingLlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,885,376 || all params: 7,511,879,680 || trainable%: 0.0917\n"
     ]
    }
   ],
   "source": [
    "from transformers.data import DataCollatorWithPadding\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    #bnb_4bit_use_double_quant=True,\n",
    "    #bnb_4bit_quant_type=\"nf4\",\n",
    "    #bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = UnmaskingLlamaForSequenceClassification.from_pretrained(\n",
    "    HF_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    num_labels=len(labels),\n",
    "    token=HF_TOKEN\n",
    ").bfloat16()\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "METRIC_NAME = \"accuracy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{CHECKPOINT_PATH}/{MODEL_NAME}\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=20,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=METRIC_NAME,\n",
    "    seed=SEED,\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "def compute_metrics(pred: EvalPrediction):\n",
    "    labels = pred.label_ids\n",
    "    accuracy = accuracy_score(labels, pred.predictions.argmax(-1))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred.predictions.argmax(-1), average=\"weighted\")\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# manual evaluation to show classifcation_report\n",
    "true_labels = []\n",
    "logits = []\n",
    "\n",
    "for batch in encoded_dataset[\"test\"]:\n",
    "    batch = {k: v.to(trainer.args.device).unsqueeze(0) for k, v in batch.items()}\n",
    "    label = batch.pop(\"label\")\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        out = model(**batch)\n",
    "\n",
    "    true_labels.append(label.item())\n",
    "    logits.extend(out.logits.tolist())\n",
    "\n",
    "probabilites = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
    "pred_labels = torch.argmax(probabilites, dim=-1).tolist()\n",
    "\n",
    "report = classification_report(true_labels, pred_labels, target_names=[f\"SDG {id2label[i]}\" for i in range(len(labels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       SDG 1       0.71      0.71      0.71        17\n",
      "      SDG 10       0.50      0.41      0.45        17\n",
      "      SDG 11       0.82      0.82      0.82        17\n",
      "      SDG 12       0.71      0.59      0.65        17\n",
      "      SDG 13       0.83      0.88      0.86        17\n",
      "      SDG 14       1.00      1.00      1.00        17\n",
      "      SDG 15       0.80      0.94      0.86        17\n",
      "      SDG 16       0.55      0.65      0.59        17\n",
      "      SDG 17       0.00      0.00      0.00         1\n",
      "       SDG 2       0.68      0.81      0.74        16\n",
      "       SDG 3       0.80      0.94      0.86        17\n",
      "       SDG 4       0.80      0.94      0.86        17\n",
      "       SDG 5       0.74      1.00      0.85        17\n",
      "       SDG 6       0.89      1.00      0.94        17\n",
      "       SDG 7       1.00      0.65      0.79        17\n",
      "       SDG 8       0.78      0.44      0.56        16\n",
      "       SDG 9       0.83      0.59      0.69        17\n",
      "\n",
      "    accuracy                           0.77       271\n",
      "   macro avg       0.73      0.73      0.72       271\n",
      "weighted avg       0.78      0.77      0.76       271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model for inference below\n",
    "\n",
    "model = UnmaskingLlamaForSequenceClassification.from_pretrained(\n",
    "    f\"{CHECKPOINT_PATH}/{MODEL_NAME}/checkpoint-1850\",\n",
    "    num_labels=len(labels),\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
