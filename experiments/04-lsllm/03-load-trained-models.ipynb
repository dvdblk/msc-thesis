{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ipynb_util_tars.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different ways to load the models\n",
    "\n",
    "the outcome of this jupynb is an easy way to access the finetuned models via variables:\n",
    "* `scibert_model` = SciBERT finetuned on ZO_UP\n",
    "* `llama_model` = LLaMA-3 finetuned on ZO_UP with a classification head\n",
    "* `unllama_model` = LLaMA-3 finetuned on ZO_UP with a classification head without causal mask\n",
    "\n",
    "### Dataset + encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SDG': ClassLabel(names=['1', '10', '11', '12', '13', '14', '15', '16', '17', '2', '3', '4', '5', '6', '7', '8', '9'], id=None), 'ABSTRACT': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'sdg_desc_short': Value(dtype='string', id=None), 'sdg_desc_long': Value(dtype='string', id=None)}\n",
      "Example instance:\t {'SDG': 16, 'ABSTRACT': 'The first attempts to modernize simply replaced the single huge engine with a huge electric motor, changing little. The drive-shafts were replaced by wires, the huge steam engine by dozens of small motors. Factories spread out, there was natural light, and room to use ceiling-slung cranes. Workers had responsibility for their own machines, they needed better training and better pay. The electric motor was a wonderful invention, once we changed all the everyday details that surrounded it.', 'id': None, 'sdg_desc_short': None, 'sdg_desc_long': None}\n",
      "Encoded (label2id) label:\t 16\n",
      "Decoded (id2label) label:\t 9\n",
      "9 16 16\n"
     ]
    }
   ],
   "source": [
    "%run ../ipynb_load_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"Is this about poverty?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import datasets\n",
    "import evaluate\n",
    "from evaluate import evaluator, Metric\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "class MulticlassAccuracy(Metric):\n",
    "    \"\"\"Workaround for the default Accuracy class which doesn't support passing 'average' to the compute method.\"\"\"\n",
    "\n",
    "    def _info(self):\n",
    "        return evaluate.MetricInfo(\n",
    "            description=\"Accuracy\",\n",
    "            citation=\"\",\n",
    "            inputs_description=\"\",\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets.Sequence(datasets.Value(\"int32\")),\n",
    "                    \"references\": datasets.Sequence(datasets.Value(\"int32\")),\n",
    "                }\n",
    "                if self.config_name == \"multilabel\"\n",
    "                else {\n",
    "                    \"predictions\": datasets.Value(\"int32\"),\n",
    "                    \"references\": datasets.Value(\"int32\"),\n",
    "                }\n",
    "            ),\n",
    "            reference_urls=[\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\"],\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references, normalize=True, sample_weight=None, **kwargs):\n",
    "        # take **kwargs to avoid breaking when the metric is used with a compute method that takes additional arguments\n",
    "        return {\n",
    "            \"accuracy\": float(\n",
    "                accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight)\n",
    "            )\n",
    "        }\n",
    "\n",
    "task_evaluator = evaluator(\"text-classification\")\n",
    "task_evaluator.METRIC_KWARGS = {\"average\": \"weighted\"}\n",
    "metrics_dict = {\n",
    "    \"accuracy\": MulticlassAccuracy(),\n",
    "    \"precision\": \"precision\",\n",
    "    \"recall\": \"recall\",\n",
    "    \"f1\": \"f1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciBERT baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([0.8610], device='cuda:0', grad_fn=<MaxBackward0>),\n",
      "indices=tensor([0], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "SCIBERT_PATH = CHECKPOINT_PATH + \"/allenai/scibert_scivocab_uncased-ft-zo_up-lower/checkpoint-240/\"\n",
    "\n",
    "scibert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    SCIBERT_PATH,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(\"cuda\")\n",
    "scibert_tokenizer = AutoTokenizer.from_pretrained(SCIBERT_PATH)\n",
    "scibert_model.eval()\n",
    "\n",
    "# Sample input to SciBERT\n",
    "sample_input = scibert_tokenizer(sample_sentence, return_tensors=\"pt\").to(\"cuda\")\n",
    "sample_output = scibert_model(**sample_input)\n",
    "print(torch.max(torch.softmax(sample_output.logits, dim=-1), dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7269372693726938,\n",
      " 'f1': 0.718275735363878,\n",
      " 'latency_in_seconds': 0.006735964851776942,\n",
      " 'precision': 0.7210623353133084,\n",
      " 'recall': 0.7269372693726938,\n",
      " 'samples_per_second': 148.45683164991587,\n",
      " 'total_time_in_seconds': 1.8254464748315513}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate SciBERT\n",
    "eval_results = task_evaluator.compute(\n",
    "    scibert_model,\n",
    "    input_column=\"ABSTRACT\",\n",
    "    label_column=\"SDG\",\n",
    "    tokenizer=scibert_tokenizer,\n",
    "    data=dataset[\"test\"],\n",
    "    label_mapping=label2id,\n",
    "    metric=evaluate.combine(metrics_dict)\n",
    ")\n",
    "pprint.pprint(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <enum 'DatasetType'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <enum 'DatasetType'>: __main__.DatasetType has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate SciBERT (manual)\n",
    "scibert_tokenized_dataset = dataset.map(\n",
    "    preprocess_data(scibert_tokenizer, include_labels=False), batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "scibert_tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# out_scibert = scibert_model(**scibert_tokenized_dataset[\"test\"][:])\n",
    "out_scibert = scibert_model(scibert_tokenized_dataset[\"test\"][:2][\"input_ids\"].to(\"cuda\"))\n",
    "pred_probs_scibert, preds_scibert = torch.max(torch.softmax(out_scibert.logits, dim=-1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0369, 0.0284, 0.0853, 0.1499, 0.0247, 0.0976, 0.1246, 0.0540, 0.0385,\n",
      "        0.0521, 0.0506, 0.0236, 0.0583, 0.0420, 0.0794, 0.0285, 0.0256],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-0.8543, -0.2057,  0.9500,  4.5180, -1.3699, -0.5298, -0.6222, -0.1837,\n",
      "         -0.4709,  0.5070, -0.0690, -1.1635, -0.2762,  0.4362,  0.6243,  0.0282,\n",
      "          0.7284]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "torch.return_types.max(\n",
      "values=tensor([0.8448], device='cuda:0', grad_fn=<MaxBackward0>),\n",
      "indices=tensor([3], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(torch.softmax(out_scibert.logits[0], dim=-1))\n",
    "\n",
    "sample_input = scibert_tokenizer(dataset[\"test\"][0][\"ABSTRACT\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "sample_output = scibert_model(**sample_input)\n",
    "print(sample_output.logits)\n",
    "print(torch.max(torch.softmax(sample_output.logits, dim=-1), dim=-1))\n",
    "\n",
    "\n",
    "sample_input2 = dataset.map(\n",
    "    preprocess_data(scibert_tokenizer, include_labels=False), batched=True, remove_columns=dataset[\"train\"].column_names, batch_size=32\n",
    ")\n",
    "sample_input2.set_format(\"torch\")\n",
    "sample_output2 = torch.tensor([])\n",
    "for batch in sample_input2[\"test\"]:\n",
    "    sample_output2 = torch.cat(\n",
    "        (\n",
    "            sample_output2,\n",
    "            scibert_model(\n",
    "                input_ids=batch[\"input_ids\"].to(\"cuda\").unsqueeze(0),\n",
    "                attention_mask=batch[\"attention_mask\"].to(\"cuda\").unsqueeze(0)\n",
    "            ).logits.detach().cpu()\n",
    "        )\n",
    "    )\n",
    "\n",
    "# sample_input2.set_format(\"torch\")\n",
    "# sample_output2 = scibert_model(\n",
    "#     input_ids=sample_input2[\"test\"][:128][\"input_ids\"].to(\"cuda\"),\n",
    "#     attention_mask=sample_input2[\"test\"][:128][\"attention_mask\"].to(\"cuda\")\n",
    "# )\n",
    "\n",
    "#print(sample_input.keys(), sample_input2[\"test\"][0][\"input_ids\"].unsqueeze(0).to(\"cuda\").shape)\n",
    "\n",
    "#print(sample_output2.logits.shape)\n",
    "#print(torch.max(torch.softmax(sample_output2.logits, dim=-1), dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7269372693726938\n",
      "[3, 9, 4, 9, 14, 15, 1, 5, 5, 10, 11, 12, 15, 14, 14, 10, 0, 14, 4, 1, 7, 1, 6, 11, 7, 10, 1, 1, 9, 9, 2, 14, 16, 15, 6, 15, 11, 10, 5, 6, 4, 1, 3, 4, 9, 9, 7, 7, 6, 14, 10, 7, 9, 12, 6, 12, 11, 12, 10, 7, 16, 14, 0, 7]\n"
     ]
    }
   ],
   "source": [
    "#scibert_accuracy = accuracy_score(y_true=dataset[\"test\"][\"SDG\"][:64], y_pred=preds_scibert.cpu())\n",
    "_, preds = torch.max(torch.softmax(sample_output2, dim=-1), dim=-1)\n",
    "scibert_accuracy = accuracy_score(y_true=dataset[\"test\"][\"SDG\"], y_pred=preds.cpu())\n",
    "print(scibert_accuracy)\n",
    "\n",
    "print(dataset[\"test\"][\"SDG\"][:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ef035a47f74cab942cf6dda86e1a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification\n",
    "\n",
    "LLAMA_PATH = f\"{CHECKPOINT_PATH}/meta-llama/Meta-Llama-3-8B-ft-zo_up/checkpoint-2200/\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(LLAMA_PATH)\n",
    "llama_model = LlamaForSequenceClassification.from_pretrained(\n",
    "    LLAMA_PATH,\n",
    "    num_labels=17,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "llama_model.config.pad_token_id = llama_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sample = llama_tokenizer(sample_sentence, return_tensors=\"pt\")\n",
    "token_ids = tokenized_sample[\"input_ids\"]\n",
    "\n",
    "out = llama_model(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.4863], dtype=torch.bfloat16),\n",
       "indices=tensor([12]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.softmax(out.logits, dim=-1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <enum 'DatasetType'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <enum 'DatasetType'>: __main__.DatasetType has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "torch.Size([1, 522])\n"
     ]
    }
   ],
   "source": [
    "# Evaluate LLaMA - can't use task evaluator because it doesn't support accelerate which is required for inference larger models\n",
    "# https://github.com/huggingface/evaluate/issues/487\n",
    "\n",
    "# tokenize the dataset first\n",
    "llama_tokenized_dataset = dataset.map(\n",
    "    preprocess_data(llama_tokenizer, padding=\"longest\", max_length=1024, include_labels=False), batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "llama_tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "print(token_ids.shape)\n",
    "print(llama_tokenized_dataset[\"test\"][0][\"input_ids\"].unsqueeze(dim=0).shape)\n",
    "\n",
    "# need  to split the input_ids tensor into two tensors to avoid CUDA out of memory error\n",
    "out = llama_model(**llama_tokenized_dataset[\"test\"][:128])\n",
    "out2 = llama_model(**llama_tokenized_dataset[\"test\"][128:])\n",
    "out = torch.cat((out.logits, out2.logits), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7269372693726938\n"
     ]
    }
   ],
   "source": [
    "pred_probs, preds = torch.max(torch.softmax(out, dim=-1), dim=-1)\n",
    "true_labels = dataset[\"test\"][\"SDG\"]\n",
    "\n",
    "# try older tf version, tinyllama (or any model llama2)\n",
    "llama_accuracy = accuracy_score(true_labels, preds)\n",
    "print(llama_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
