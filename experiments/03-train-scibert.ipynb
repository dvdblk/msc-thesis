{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCIBERT\n",
    "\n",
    "trained on Zora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from spacy.cli.train import train as spacy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = 'zora'\n",
    "test_set = 'zora'\n",
    "model_name = 'scibert'\n",
    "\n",
    "# Edit paths as needed:\n",
    "config_path = f\"../cfg/spacy/{model_name}.cfg\"\n",
    "model_out_path = f\"models/ft-{model_name}-{train_set}\"\n",
    "train_path, dev_path, test_path = (\n",
    "    f\"data/spacy_docs/{train_set}_train.spacy\",\n",
    "    f\"data/spacy_docs/{train_set}_dev.spacy\",\n",
    "    f\"data/spacy_docs/{test_set}_test.spacy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: models/ft-scibert-zora\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  -------------  ------------  ----------  ------\n",
      "  0       0           0.00          0.06        0.37    0.00\n",
      "  6     200           0.00         37.32       17.16    0.17\n",
      " 12     400           0.02         18.98       33.56    0.34\n",
      " 18     600           0.02          5.23       29.39    0.29\n",
      " 25     800           0.02          1.73       28.24    0.28\n",
      " 31    1000           0.01          0.83       32.78    0.33\n",
      " 37    1200           0.01          0.44       31.44    0.31\n",
      " 43    1400           0.01          0.13       32.00    0.32\n",
      " 50    1600           0.02          0.11       31.44    0.31\n",
      " 56    1800           0.01          0.09       31.44    0.31\n",
      " 62    2000           0.01          0.09       31.44    0.31\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "models/ft-scibert-zora/model-last\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "spacy_train(\n",
    "    config_path,\n",
    "    output_path=model_out_path,\n",
    "    use_gpu=0,\n",
    "    overrides={\n",
    "        \"paths.train\": train_path,\n",
    "        \"paths.dev\": dev_path\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK                 100.00\n",
      "TEXTCAT (macro F)   38.61 \n",
      "SPEED               3911  \n",
      "\n",
      "\u001b[1m\n",
      "=========================== Textcat F (per label) ===========================\u001b[0m\n",
      "\n",
      "          P        R        F\n",
      "1      0.00     0.00     0.00\n",
      "2     50.00    50.00    50.00\n",
      "3     75.00    92.31    82.76\n",
      "4      0.00     0.00     0.00\n",
      "5     77.78   100.00    87.50\n",
      "6      0.00     0.00     0.00\n",
      "7    100.00   100.00   100.00\n",
      "8     50.00    87.50    63.64\n",
      "9      0.00     0.00     0.00\n",
      "10    41.67    50.00    45.45\n",
      "11     0.00     0.00     0.00\n",
      "12    33.33    44.44    38.10\n",
      "13   100.00    58.33    73.68\n",
      "14     0.00     0.00     0.00\n",
      "15    72.73    88.89    80.00\n",
      "16    42.86    30.00    35.29\n",
      "17     0.00     0.00     0.00\n",
      "\n",
      "\u001b[1m\n",
      "======================== Textcat ROC AUC (per label) ========================\u001b[0m\n",
      "\n",
      "     ROC AUC\n",
      "1       0.87\n",
      "2       0.84\n",
      "3       0.98\n",
      "4       1.00\n",
      "5       1.00\n",
      "6       0.92\n",
      "7       1.00\n",
      "8       0.94\n",
      "9       0.88\n",
      "10      0.85\n",
      "11      0.43\n",
      "12      0.67\n",
      "13      0.96\n",
      "14      0.59\n",
      "15      0.95\n",
      "16      0.82\n",
      "17      0.91\n",
      "\n",
      "\u001b[38;5;2m✔ Saved results to\n",
      "models/ft-scibert-zora/model-best/test_zora_eval.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "best_model = f\"models/ft-scibert-zora/model-best\"\n",
    "test_cats = [str(i) for i in range(1, 18)]\n",
    "output_file = f\"{best_model}/test_{test_set}_eval.json\"\n",
    "\n",
    "!python3 -m spacy benchmark accuracy --gpu-id 0 \\\n",
    "  {best_model}/ {test_path} \\\n",
    "  --output {output_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions....\n",
      "Done making predictions!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_test_df(file):\n",
    "  test_df = pd.read_csv(file, sep='\\t', keep_default_na=False,\n",
    "                        index_col=0, encoding='utf-8')\n",
    "  test_df = test_df.astype({'sdg': 'string',\n",
    "                            'abstract': 'string'})\n",
    "  test_df.drop(columns=['faculty', 'year'], inplace=True)\n",
    "\n",
    "  return test_df\n",
    "\n",
    "# Edit `test_df` path:\n",
    "test_df = get_test_df(f\"data/train_test/{test_set}_test_clean.tsv\")\n",
    "\n",
    "X_test = test_df['abstract'].values\n",
    "y_test = test_df['sdg'].values\n",
    "\n",
    "nlp = spacy.load(best_model)\n",
    "print(\"Making predictions....\")\n",
    "\n",
    "spacy_probs = [doc.cats for doc in nlp.pipe(X_test)]\n",
    "print(\"Done making predictions!\")\n",
    "\n",
    "# For each item, select the label to which the model has assigned the highest probability:\n",
    "preds = []\n",
    "probs = []\n",
    "for label_probs_dict in spacy_probs:\n",
    "    pred, prob = max(label_probs_dict.items(), key=lambda x: x[1])\n",
    "    preds.append(pred)\n",
    "    probs.append(prob)\n",
    "\n",
    "preds = pd.Series(preds)\n",
    "probs = pd.Series(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame({'abstract': X_test,\n",
    "                         'label': y_test,\n",
    "                         'prediction': preds,\n",
    "                         'probability': probs})\n",
    "# Align original indices\n",
    "preds_df.index = test_df.index\n",
    "\n",
    "preds_df = preds_df.astype({'abstract': 'string',\n",
    "                            'label': 'int',\n",
    "                            'prediction': 'int',\n",
    "                            'probability': 'float'})\n",
    "\n",
    "preds_file = f\"{train_set}-{test_set}_preds.tsv\"\n",
    "preds_df.to_csv(preds_file, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
