{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ipynb_util_tars.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SDG': Value(dtype='int64', id=None), 'ABSTRACT': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'sdg_desc_short': Value(dtype='string', id=None), 'sdg_desc_long': Value(dtype='string', id=None), '__index_level_0__': Value(dtype='int64', id=None)}\n",
      "Example instance:\t {'SDG': 8, 'ABSTRACT': 'The scheme gives enterprises with business activity in Norway a tax credit on their R&D projects. The R&D content must be approved by the Research Council of Norway ex ante. In 2009, the cap on expenses per enterprise for intramural R&D projects increased to NOK 5.5 million (previously it was N0K 4 million), and NOK11 million (previously it was NOK 8 million) for projects conducted at an R&D institution.', 'id': None, 'sdg_desc_short': None, 'sdg_desc_long': None, '__index_level_0__': 492}\n",
      "id2label: {0: '1', 1: '2', 2: '3', 3: '4', 4: '5', 5: '6', 6: '7', 7: '8', 8: '9', 9: '10', 10: '11', 11: '12', 12: '13', 13: '14', 14: '15', 15: '16', 16: '17'}\n",
      "label2id: {'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7, '9': 8, '10': 9, '11': 10, '12': 11, '13': 12, '14': 13, '15': 14, '16': 15, '17': 16}\n",
      "Encoded (label2id) label:\t 8\n",
      "Decoded (id2label) label:\t 9\n",
      "17 16 8\n"
     ]
    }
   ],
   "source": [
    "%run ../ipynb_load_data_natural.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac43df8d7674dbdbead51ee9c166a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <enum 'DatasetType'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <enum 'DatasetType'>: __main__.DatasetType has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aadb77a27f0460d8f11b012012ce16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a247d263587e46b09659527bc73632b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/271 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       SDG 1     0.7368    0.8235    0.7778        17\n",
      "       SDG 2     0.9375    0.8824    0.9091        17\n",
      "       SDG 3     0.7500    0.8824    0.8108        17\n",
      "       SDG 4     1.0000    1.0000    1.0000        17\n",
      "       SDG 5     0.7647    0.7647    0.7647        17\n",
      "       SDG 6     0.9412    1.0000    0.9697        16\n",
      "       SDG 7     0.8000    0.7500    0.7742        16\n",
      "       SDG 8     0.7778    0.8235    0.8000        17\n",
      "       SDG 9     0.7059    0.7059    0.7059        17\n",
      "      SDG 10     0.5385    0.4118    0.4667        17\n",
      "      SDG 11     0.8889    0.9412    0.9143        17\n",
      "      SDG 12     0.9231    0.7059    0.8000        17\n",
      "      SDG 13     0.7778    0.8235    0.8000        17\n",
      "      SDG 14     1.0000    1.0000    1.0000        17\n",
      "      SDG 15     0.8125    0.7647    0.7879        17\n",
      "      SDG 16     0.6842    0.7647    0.7222        17\n",
      "      SDG 17     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.8118       271\n",
      "   macro avg     0.7670    0.7673    0.7649       271\n",
      "weighted avg     0.8115    0.8118    0.8093       271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification\n",
    "\n",
    "LLAMA_PATH = CHECKPOINT_PATH + \"/final/meta-llama/Meta-Llama-3-8B-ft-zo_up/checkpoint-2212\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(LLAMA_PATH)\n",
    "llama_model = LlamaForSequenceClassification.from_pretrained(\n",
    "    LLAMA_PATH,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "llama_model.eval()\n",
    "llama_model.config.pad_token_id = llama_tokenizer.pad_token_id\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_data(llama_tokenizer, max_length=1024, padding=\"longest\"), batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "encoded_dataset.set_format(\"torch\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# manual evaluation to show classifcation_report\n",
    "true_labels = []\n",
    "logits = []\n",
    "\n",
    "for batch in encoded_dataset[\"test\"]:\n",
    "    batch = {k: v.to(llama_model.device).unsqueeze(0) for k, v in batch.items()}\n",
    "    label = batch.pop(\"label\")\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        out = llama_model(**batch)\n",
    "\n",
    "    true_labels.append(label.item())\n",
    "    logits.extend(out.logits.tolist())\n",
    "\n",
    "probabilites = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
    "pred_labels = torch.argmax(probabilites, dim=-1).tolist()\n",
    "\n",
    "report = classification_report(true_labels, pred_labels, target_names=[f\"SDG {id2label[i]}\" for i in range(len(labels))], digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdvdblk\u001b[0m (\u001b[33mngmi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/dbielik/msc-thesis/experiments/07-finalized-ft/wandb/run-20240803_145350-5104ow20</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ngmi/huggingface/runs/5104ow20' target=\"_blank\">./eval_output</a></strong> to <a href='https://wandb.ai/ngmi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ngmi/huggingface' target=\"_blank\">https://wandb.ai/ngmi/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ngmi/huggingface/runs/5104ow20' target=\"_blank\">https://wandb.ai/ngmi/huggingface/runs/5104ow20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9594660401344299, 'eval_accuracy': 0.8081180811808119, 'eval_precision': 0.8060808665957582, 'eval_recall': 0.8081180811808119, 'eval_f1': 0.8050867543472591, 'eval_runtime': 40.9142, 'eval_samples_per_second': 6.624, 'eval_steps_per_second': 1.662}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compute_metrics(pred: EvalPrediction):\n",
    "    labels = pred.label_ids\n",
    "    accuracy = accuracy_score(labels, pred.predictions.argmax(-1))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred.predictions.argmax(-1), average=\"weighted\")\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "eval_trainer = Trainer(\n",
    "    model=llama_model,\n",
    "    args=TrainingArguments(output_dir=\"./eval_output\", per_device_eval_batch_size=4),\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "eval_results = eval_trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 14,\n",
       " 11,\n",
       " 8,\n",
       " 1,\n",
       " 11,\n",
       " 0,\n",
       " 3,\n",
       " 16,\n",
       " 8,\n",
       " 2,\n",
       " 13,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 15,\n",
       " 4,\n",
       " 14,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 11,\n",
       " 15,\n",
       " 15,\n",
       " 13,\n",
       " 11,\n",
       " 3,\n",
       " 10,\n",
       " 10,\n",
       " 13,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 10,\n",
       " 10,\n",
       " 1,\n",
       " 2,\n",
       " 14,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 12,\n",
       " 4,\n",
       " 10,\n",
       " 11,\n",
       " 2,\n",
       " 15,\n",
       " 12,\n",
       " 6,\n",
       " 0,\n",
       " 13,\n",
       " 11,\n",
       " 12,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 14,\n",
       " 3,\n",
       " 5,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 12,\n",
       " 7,\n",
       " 12,\n",
       " 2,\n",
       " 10,\n",
       " 14,\n",
       " 14,\n",
       " 10,\n",
       " 15,\n",
       " 9,\n",
       " 11,\n",
       " 3,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 13,\n",
       " 15,\n",
       " 9,\n",
       " 10,\n",
       " 5,\n",
       " 4,\n",
       " 13,\n",
       " 12,\n",
       " 8,\n",
       " 2,\n",
       " 10,\n",
       " 0,\n",
       " 15,\n",
       " 11,\n",
       " 14,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 14,\n",
       " 1,\n",
       " 12,\n",
       " 10,\n",
       " 14,\n",
       " 4,\n",
       " 15,\n",
       " 0,\n",
       " 6,\n",
       " 13,\n",
       " 14,\n",
       " 2,\n",
       " 10,\n",
       " 9,\n",
       " 12,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 13,\n",
       " 5,\n",
       " 13,\n",
       " 15,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 13,\n",
       " 6,\n",
       " 15,\n",
       " 13,\n",
       " 4,\n",
       " 1,\n",
       " 12,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 12,\n",
       " 5,\n",
       " 15,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 15,\n",
       " 7,\n",
       " 15,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 10,\n",
       " 3,\n",
       " 4,\n",
       " 13,\n",
       " 10,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 11,\n",
       " 8,\n",
       " 13,\n",
       " 14,\n",
       " 8,\n",
       " 15,\n",
       " 12,\n",
       " 0,\n",
       " 14,\n",
       " 9,\n",
       " 13,\n",
       " 4,\n",
       " 10,\n",
       " 12,\n",
       " 2,\n",
       " 8,\n",
       " 12,\n",
       " 12,\n",
       " 1,\n",
       " 15,\n",
       " 1,\n",
       " 13,\n",
       " 11,\n",
       " 13,\n",
       " 8,\n",
       " 10,\n",
       " 6,\n",
       " 15,\n",
       " 14,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 12,\n",
       " 11,\n",
       " 5,\n",
       " 14,\n",
       " 10,\n",
       " 12,\n",
       " 10,\n",
       " 0,\n",
       " 15,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 11,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 10,\n",
       " 1,\n",
       " 15,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 12]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <enum 'DatasetType'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <enum 'DatasetType'>: __main__.DatasetType has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d68328be6549aba15627bc0ac4256f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e64b6e39e849259d07e2b47e964a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/271 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       SDG 1     0.8235    0.8235    0.8235        17\n",
      "       SDG 2     0.7727    1.0000    0.8718        17\n",
      "       SDG 3     0.7368    0.8235    0.7778        17\n",
      "       SDG 4     0.8889    0.9412    0.9143        17\n",
      "       SDG 5     0.7857    0.6471    0.7097        17\n",
      "       SDG 6     0.8421    1.0000    0.9143        16\n",
      "       SDG 7     0.6316    0.7500    0.6857        16\n",
      "       SDG 8     0.6875    0.6471    0.6667        17\n",
      "       SDG 9     0.6250    0.5882    0.6061        17\n",
      "      SDG 10     0.5333    0.4706    0.5000        17\n",
      "      SDG 11     0.8824    0.8824    0.8824        17\n",
      "      SDG 12     0.7273    0.4706    0.5714        17\n",
      "      SDG 13     0.8000    0.7059    0.7500        17\n",
      "      SDG 14     0.8889    0.9412    0.9143        17\n",
      "      SDG 15     0.8571    0.7059    0.7742        17\n",
      "      SDG 16     0.5714    0.7059    0.6316        17\n",
      "      SDG 17     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.7528       271\n",
      "   macro avg     0.7091    0.7119    0.7055       271\n",
      "weighted avg     0.7507    0.7528    0.7465       271\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/user/dbielik/msc-thesis/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# load scibert\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# load the best model\n",
    "MODEL_PATH = CHECKPOINT_PATH + \"/final/allenai/scibert_scivocab_cased-zo_up/checkpoint-432/\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, do_lower_case=False, skip_special_tokens=False)\n",
    "model.eval()\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_data(tokenizer, max_length=512, padding=True), batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "encoded_dataset.set_format(\"torch\")\n",
    "\n",
    "# manual evaluation to show classifcation_report\n",
    "true_labels = []\n",
    "logits = []\n",
    "\n",
    "for batch in encoded_dataset[\"test\"]:\n",
    "    batch = {k: v.to(model.device).unsqueeze(0) for k, v in batch.items()}\n",
    "    label = batch.pop(\"label\")\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        out = model(**batch)\n",
    "\n",
    "    true_labels.append(label.item())\n",
    "    logits.extend(out.logits.tolist())\n",
    "\n",
    "probabilites = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
    "pred_labels = torch.argmax(probabilites, dim=-1).tolist()\n",
    "\n",
    "report = classification_report(true_labels, pred_labels, target_names=[f\"SDG {id2label[i]}\" for i in range(len(labels))], digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 6, 5, 3, 8, 5, 14, 15, 8, 1, 11, 0, 3, 14, 8, 2, 13, 7, 4, 0, 2, 0, 0, 15, 15, 7, 14, 0, 4, 9, 11, 15, 9, 13, 11, 3, 10, 10, 13, 5, 0, 1, 3, 0, 10, 10, 1, 4, 14, 9, 8, 7, 2, 0, 1, 11, 4, 10, 11, 1, 15, 12, 6, 0, 13, 11, 6, 9, 1, 7, 1, 3, 13, 6, 5, 4, 5, 15, 3, 5, 3, 5, 12, 13, 14, 14, 12, 7, 6, 2, 10, 14, 14, 6, 15, 9, 8, 3, 2, 2, 4, 6, 0, 13, 15, 11, 10, 5, 4, 13, 12, 8, 2, 10, 0, 15, 11, 14, 1, 8, 7, 2, 7, 5, 3, 3, 6, 7, 14, 1, 12, 10, 5, 2, 15, 0, 6, 13, 14, 2, 10, 3, 14, 13, 1, 4, 9, 7, 5, 5, 8, 3, 13, 5, 13, 15, 7, 4, 4, 2, 6, 7, 2, 13, 6, 9, 13, 4, 1, 12, 2, 0, 6, 0, 1, 9, 12, 5, 15, 12, 5, 6, 3, 7, 15, 7, 15, 2, 2, 9, 1, 5, 7, 1, 10, 3, 4, 13, 10, 9, 10, 5, 6, 9, 15, 2, 6, 8, 13, 14, 8, 15, 12, 0, 8, 9, 12, 4, 9, 6, 2, 15, 11, 12, 1, 15, 1, 13, 11, 13, 8, 10, 6, 8, 1, 8, 9, 0, 6, 4, 12, 11, 5, 14, 10, 12, 10, 1, 15, 12, 8, 9, 15, 1, 7, 8, 2, 7, 10, 1, 15, 6, 3, 3, 1, 3, 0, 1, 5, 12]\n"
     ]
    }
   ],
   "source": [
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
