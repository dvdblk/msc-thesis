{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_evaluations(base_dir=\"evals\"):\n",
    "    evaluations = {}\n",
    "    for model in os.listdir(base_dir):\n",
    "        model_path = os.path.join(base_dir, model)\n",
    "        if os.path.isdir(model_path):\n",
    "            evaluations[model] = {}\n",
    "            for file in os.listdir(model_path):\n",
    "                if file.endswith('_evaluation.npy'):\n",
    "                    xai_method = file.split('_')[0]\n",
    "                    file_path = os.path.join(model_path, file)\n",
    "                    evaluations[model][xai_method] = np.load(file_path)\n",
    "    return evaluations\n",
    "\n",
    "# Load all evaluations\n",
    "all_evaluations = load_evaluations()\n",
    "\n",
    "# Compute averages and standard deviations\n",
    "def compute_stats(eval_array):\n",
    "    return np.mean(eval_array, axis=0), np.std(eval_array, axis=0)\n",
    "\n",
    "stats = {model: {xai: compute_stats(eval_array)\n",
    "                 for xai, eval_array in model_evals.items()}\n",
    "         for model, model_evals in all_evaluations.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity / correctness check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_nan(arr):\n",
    "    return np.isnan(arr).any()\n",
    "\n",
    "for model in all_evaluations:\n",
    "    for xai_method in all_evaluations[model]:\n",
    "        # Verify no score is NaN\n",
    "        if has_nan(all_evaluations[model][xai_method]):\n",
    "            print(f\"Model {model} with XAI method {xai_method} has NaNs\")\n",
    "        # check if all values are in the range [-1, 1]\n",
    "        if (all_evaluations[model][xai_method] > 1).any() or (all_evaluations[model][xai_method] < -1).any():\n",
    "            print(f\"Model {model} with XAI method {xai_method} has values outside of [-1, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271, 17, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_evaluations[\"scibert\"][\"shap-partition\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(evaluations, model, method):\n",
    "    shap_evals = evaluations[model][method]\n",
    "    first_scores = shap_evals[:, :, 0]\n",
    "    predictions = np.argmax(first_scores, axis=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_preds=[3,6,6,5,3,8,5,14,11,8,1,11,0,3,16,8,2,13,0,4,3,2,0,0,15,15,4,14,0,4,9,11,15,15,13,11,3,10,10,13,5,0,1,3,0,10,10,1,2,14,0,8,7,0,0,1,12,4,10,11,2,15,12,6,0,13,11,12,7,0,7,1,3,7,6,5,4,5,4,3,14,3,5,12,13,14,14,12,7,12,2,10,14,14,10,15,9,11,3,9,2,2,7,0,13,15,9,10,5,4,13,12,8,2,10,0,15,11,14,1,8,7,2,7,5,3,3,6,7,14,1,12,10,14,4,15,0,6,13,14,2,10,9,12,2,1,4,7,7,5,5,8,8,13,5,13,15,7,4,2,2,6,7,8,13,6,15,13,4,1,12,2,9,6,4,1,9,12,5,15,6,5,6,3,7,15,7,15,2,8,9,2,5,7,1,10,3,4,13,10,6,0,5,11,9,9,2,11,8,13,14,8,15,12,0,14,9,13,4,10,12,2,8,12,12,1,15,1,13,11,13,8,10,6,15,14,8,9,9,6,4,12,11,5,14,10,12,10,0,15,8,7,7,11,1,4,8,2,2,10,1,15,6,3,3,1,3,0,4,5,12]\n",
    "scibert_preds=[3,2,6,5,3,8,5,14,15,8,1,11,0,3,14,8,2,13,7,4,0,2,0,0,15,15,7,14,0,4,9,11,15,9,13,11,3,10,10,13,5,0,1,3,0,10,10,1,4,14,9,8,7,2,0,1,11,4,10,11,1,15,12,6,0,13,11,6,9,1,7,1,3,13,6,5,4,5,15,3,5,3,5,12,13,14,14,12,7,6,2,10,14,14,6,15,9,8,3,2,2,4,6,0,13,15,11,10,5,4,13,12,8,2,10,0,15,11,14,1,8,7,2,7,5,3,3,6,7,14,1,12,10,5,2,15,0,6,13,14,2,10,3,14,13,1,4,9,7,5,5,8,3,13,5,13,15,7,4,4,2,6,7,2,13,6,9,13,4,1,12,2,0,6,0,1,9,12,5,15,12,5,6,3,7,15,7,15,2,2,9,1,5,7,1,10,3,4,13,10,9,10,5,6,9,15,2,6,8,13,14,8,15,12,0,8,9,12,4,9,6,2,15,11,12,1,15,1,13,11,13,8,10,6,8,1,8,9,0,6,4,12,11,5,14,10,12,10,1,15,12,8,9,15,1,7,8,2,7,10,1,15,6,3,3,1,3,0,1,5,12]\n",
    "unllama3_preds=[3,2,6,5,3,8,5,14,14,8,4,11,0,3,14,8,2,13,0,4,3,2,0,0,15,15,2,12,0,4,9,11,15,9,13,11,3,10,10,13,5,0,1,3,0,10,10,1,15,14,0,8,7,2,0,1,11,4,10,11,2,15,12,6,0,13,11,6,9,0,7,1,3,14,6,5,4,5,4,3,12,3,5,12,13,14,14,12,2,12,2,10,14,14,10,15,9,11,3,9,2,9,7,0,13,15,11,10,5,4,13,12,8,2,6,0,15,11,14,1,11,3,2,7,5,3,3,6,7,14,1,12,10,12,4,15,0,6,13,14,2,10,9,14,2,1,4,7,7,5,5,8,8,13,5,13,15,9,4,5,2,11,7,2,13,6,11,13,4,1,12,15,9,6,4,1,7,12,5,15,12,5,6,3,7,15,7,15,2,2,9,2,5,7,1,10,3,4,13,10,8,0,5,11,9,4,2,6,8,13,14,8,15,6,0,8,9,13,4,10,12,2,8,8,12,1,9,1,13,11,13,8,10,6,15,1,8,9,0,6,4,12,11,5,14,10,12,10,1,15,8,8,7,15,1,0,8,15,7,10,1,9,6,11,3,1,3,0,1,5,14]\n",
    "ground_truth=[3,6,8,5,3,8,5,14,11,8,1,11,0,3,14,8,2,13,9,4,3,2,0,0,9,8,4,12,0,4,8,11,9,15,13,6,3,10,10,13,5,0,1,3,4,10,10,1,2,14,9,8,7,0,0,1,11,4,10,11,1,15,14,6,0,13,11,12,9,15,7,1,3,14,6,5,4,5,4,3,14,3,5,12,13,14,14,12,7,12,2,10,14,14,6,15,9,11,3,9,2,4,7,0,13,15,15,10,5,11,13,12,8,2,10,0,15,11,14,1,11,7,2,7,5,3,3,6,7,14,1,12,10,14,4,15,0,6,13,14,2,10,9,14,2,1,7,16,7,5,5,8,8,13,6,13,15,9,4,4,2,6,7,2,13,6,15,13,4,1,12,15,0,6,4,1,9,12,5,15,12,5,6,3,7,9,7,15,2,2,11,2,5,7,1,10,3,4,13,10,9,0,5,11,9,4,2,11,8,13,12,8,15,12,0,6,9,13,4,10,12,2,15,8,12,1,15,1,13,11,13,9,10,6,10,14,8,9,0,6,4,12,11,5,14,10,12,8,7,15,8,7,7,11,1,0,8,2,9,10,7,11,6,3,3,1,3,0,1,5,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=8, predicted_label=14, highest scoring label=15\n",
      "[-0.0003 -0.0002 -0.1582 -0.162  -0.2492 -0.0034 -0.0004 -0.0012  0.0007\n",
      " -0.0042 -0.0674  0.2874 -0.0005 -0.2856 -0.0031  0.6037  0.0027]\n",
      "i=9, predicted_label=8, highest scoring label=7\n",
      "[-0.028  -0.0074 -0.0001 -0.0242 -0.0016 -0.001  -0.0004  0.0334  0.7483\n",
      "  0.0013  0.0018 -0.0007  0.0001 -0.0001 -0.0001 -0.0047 -0.    ]\n",
      "i=27, predicted_label=12, highest scoring label=14\n",
      "[-0.     -0.1235 -0.0042 -0.0018 -0.     -0.0025 -0.001  -0.0002 -0.0014\n",
      " -0.0007 -0.0317 -0.1342  0.5235 -0.0001  0.3686 -0.0001 -0.0001]\n",
      "i=33, predicted_label=9, highest scoring label=7\n",
      "[ 0.0001 -0.0046 -0.0001 -0.0443  0.0013 -0.     -0.0001  0.0685 -0.0021\n",
      " -0.0846 -0.0706 -0.0001  0.0001 -0.0009  0.      0.7859  0.0021]\n",
      "i=34, predicted_label=13, highest scoring label=5\n",
      "[-0.     -0.0001 -0.0165 -0.0001 -0.0002 -0.     -0.     -0.0006 -0.0272\n",
      " -0.     -0.0001 -0.0003 -0.      0.4322 -0.     -0.0037 -0.    ]\n",
      "i=38, predicted_label=10, highest scoring label=15\n",
      "[-0.0001 -0.     -0.0007 -0.2788 -0.     -0.003  -0.0001  0.0001 -0.001\n",
      " -0.      0.5682 -0.0005 -0.1098 -0.     -0.002   0.     -0.    ]\n",
      "i=48, predicted_label=15, highest scoring label=4\n",
      "[ 0.0002 -0.115   0.7865 -0.0891  0.0377 -0.0002 -0.0006  0.0901 -0.2252\n",
      " -0.0038 -0.     -0.0585 -0.0022 -0.0013 -0.002  -0.0028 -0.0005]\n",
      "i=51, predicted_label=8, highest scoring label=7\n",
      "[-0.0403 -0.1086 -0.0069 -0.0791 -0.0001 -0.0014  0.0034  0.1135  0.5\n",
      " -0.1438 -0.0006  0.0045  0.     -0.0029 -0.0025 -0.0036 -0.0035]\n",
      "i=52, predicted_label=7, highest scoring label=3\n",
      "[-0.0037 -0.0001 -0.0017 -0.5074 -0.0786 -0.0011 -0.0001  0.8355 -0.1348\n",
      " -0.0204 -0.0003 -0.     -0.     -0.0018 -0.0001 -0.0744 -0.    ]\n",
      "i=53, predicted_label=2, highest scoring label=0\n",
      "[ 0.7768 -0.0018 -0.0102 -0.0026 -0.     -0.0177 -0.001  -0.006  -0.0051\n",
      "  0.      0.      0.     -0.0001 -0.027  -0.     -0.0034 -0.0003]\n",
      "i=56, predicted_label=11, highest scoring label=12\n",
      "[-0.129  -0.0496 -0.1547  0.001  -0.0119 -0.0506 -0.2315 -0.3545 -0.1117\n",
      " -0.0183 -0.0082 -0.0007  0.7626 -0.0152  0.1121  0.0944 -0.3448]\n",
      "i=60, predicted_label=2, highest scoring label=0\n",
      "[ 0.0635  0.2587  0.5826 -0.0536 -0.0002 -0.0062 -0.2388 -0.0042 -0.0001\n",
      " -0.0689 -0.1193 -0.0242 -0.0417 -0.0005 -0.0027 -0.0015 -0.0019]\n",
      "i=62, predicted_label=12, highest scoring label=14\n",
      "[-0.     -0.0005 -0.0002 -0.0243 -0.0017 -0.0002 -0.0005 -0.     -0.0093\n",
      " -0.0014 -0.2329 -0.0102  0.8499 -0.0001  0.035  -0.     -0.0001]\n",
      "i=67, predicted_label=6, highest scoring label=12\n",
      "[-0.0108 -0.0003 -0.0009 -0.4113 -0.0018 -0.     -0.1467 -0.0046 -0.0226\n",
      " -0.0155 -0.0435 -0.0025  0.915  -0.     -0.0027 -0.0019 -0.0003]\n",
      "i=73, predicted_label=14, highest scoring label=7\n",
      "[-0.044  -0.3604 -0.2385 -0.2243 -0.0003 -0.7431 -0.0091  0.6838  0.\n",
      " -0.0004 -0.0806 -0.0002 -0.0011 -0.0087  0.2415 -0.0041  0.0021]\n",
      "i=75, predicted_label=5, highest scoring label=1\n",
      "[ 0.     -0.0036 -0.0033 -0.0026 -0.      0.2358 -0.      0.0003 -0.0005\n",
      " -0.0013  0.     -0.0075  0.     -0.0001 -0.     -0.     -0.0007]\n",
      "i=82, predicted_label=5, highest scoring label=15\n",
      "[ 0.0841  0.0002  0.2635  0.0199 -0.0069  0.5314  0.0004  0.0069 -0.0021\n",
      "  0.0149  0.0012 -0.0008  0.0005 -0.     -0.0001  0.0009  0.0026]\n",
      "i=96, predicted_label=9, highest scoring label=7\n",
      "[-0.2521 -0.1151 -0.093  -0.0456 -0.0063 -0.0004 -0.2459 -0.119  -0.\n",
      "  0.7827  0.     -0.     -0.     -0.0057 -0.0016 -0.1162 -0.0083]\n",
      "i=101, predicted_label=9, highest scoring label=4\n",
      "[-0.0041 -0.0862  0.562  -0.0028  0.0025 -0.0369 -0.0006 -0.0009 -0.0893\n",
      "  0.0014 -0.1169 -0.0006 -0.0028 -0.0001 -0.0002 -0.0105 -0.0039]\n",
      "i=106, predicted_label=11, highest scoring label=7\n",
      "[ 0.0005 -0.2247 -0.0006 -0.8072 -0.0376 -0.0052 -0.0334  0.25   -0.0085\n",
      "  0.3494 -0.0008  0.0162  0.0236 -0.0087  0.0003  0.0547 -0.0445]\n",
      "i=113, predicted_label=2, highest scoring label=3\n",
      "[-0.0001 -0.0129  0.3115  0.4303 -0.0005 -0.023  -0.0135 -0.0001 -0.0838\n",
      " -0.0326 -0.0256 -0.0066 -0.0005 -0.     -0.0015  0.0203 -0.0004]\n",
      "i=121, predicted_label=3, highest scoring label=15\n",
      "[ 0.0643 -0.008   0.0149  0.0303 -0.0097 -0.0028 -0.0054  0.6947 -0.0907\n",
      "  0.1189 -0.0291  0.0106 -0.0131 -0.017  -0.0013 -0.765  -0.0042]\n",
      "i=133, predicted_label=12, highest scoring label=5\n",
      "[-0.0253 -0.0042  0.     -0.0644 -0.     -0.0088 -0.0061 -0.0001 -0.0294\n",
      " -0.1094 -0.0004 -0.0002  0.0066 -0.0268  0.8829  0.     -0.0007]\n",
      "i=142, predicted_label=9, highest scoring label=3\n",
      "[ 0.0673 -0.0001 -0.0001 -0.     -0.0527 -0.0109 -0.      0.0001 -0.\n",
      "  0.6334  0.     -0.0001 -0.0001 -0.0023 -0.     -0.0003 -0.    ]\n",
      "i=143, predicted_label=14, highest scoring label=12\n",
      "[-0.0001  0.0044 -0.0008 -0.0246 -0.0023 -0.0087 -0.0074 -0.0384 -0.4029\n",
      " -0.0027 -0.0764 -0.003   0.5072 -0.0064  0.4196 -0.0015 -0.2845]\n",
      "i=147, predicted_label=7, highest scoring label=9\n",
      "[ 0.0019 -0.009  -0.0015 -0.0136  0.0019 -0.0003 -0.      0.8748 -0.2856\n",
      " -0.0797 -0.0009  0.0674 -0.     -0.     -0.0001 -0.122   0.0006]\n",
      "i=154, predicted_label=5, highest scoring label=10\n",
      "[-0.0002 -0.001  -0.0086 -0.0044 -0.0058  0.1131  0.3271  0.0004  0.0126\n",
      " -0.0008 -0.0057 -0.0134 -0.0891 -0.0035 -0.0068 -0.0066 -0.    ]\n",
      "i=163, predicted_label=2, highest scoring label=13\n",
      "[-0.008   0.0018 -0.0896 -0.0013 -0.0017 -0.1092 -0.1442 -0.0003  0.5271\n",
      " -0.1401  0.0886 -0.0323 -0.0243  0.1053 -0.0001  0.0006 -0.0108]\n",
      "i=166, predicted_label=11, highest scoring label=7\n",
      "[-0.2774 -0.0001  0.0012 -0.6648 -0.0056  0.     -0.003   0.0053  0.0014\n",
      " -0.4513 -0.0173  0.0086 -0.0003 -0.     -0.      0.9483  0.0121]\n",
      "i=185, predicted_label=15, highest scoring label=9\n",
      "[-0.0943 -0.1028  0.0075 -0.4336  0.089  -0.0379 -0.0002  0.2794 -0.0002\n",
      "  0.0434  0.0142  0.0002 -0.1409 -0.0254 -0.      0.3323 -0.0127]\n",
      "i=189, predicted_label=2, highest scoring label=8\n",
      "[-0.0008  0.4244  0.0281 -0.0147 -0.0003 -0.0052  0.0072 -0.0004  0.2944\n",
      " -0.0301  0.0047  0.0013 -0.0103 -0.0201 -0.     -0.0446 -0.0155]\n",
      "i=191, predicted_label=2, highest scoring label=1\n",
      "[-0.0089  0.0976  0.3423 -0.4945  0.0001 -0.0112  0.0008 -0.0006 -0.0135\n",
      " -0.1063 -0.0023 -0.0179 -0.0011 -0.0027  0.     -0.0004 -0.0001]\n",
      "i=197, predicted_label=4, highest scoring label=9\n",
      "[ 0.0029 -0.      0.      0.      0.867  -0.     -0.      0.0057 -0.\n",
      "  0.1241 -0.     -0.0001 -0.     -0.     -0.     -0.     -0.    ]\n",
      "i=200, predicted_label=8, highest scoring label=12\n",
      "[ 0.0028  0.0133 -0.0929 -0.1059 -0.0054 -0.0051  0.523   0.0597 -0.0078\n",
      " -0.3077 -0.003  -0.1389  0.3047 -0.      0.0244  0.0035 -0.0087]\n",
      "i=201, predicted_label=0, highest scoring label=10\n",
      "[ 0.5612  0.     -0.0003 -0.0004 -0.0108 -0.0004 -0.0001  0.0003 -0.0011\n",
      "  0.0003  0.2011  0.     -0.0228 -0.0001 -0.     -0.0207 -0.    ]\n",
      "i=204, predicted_label=9, highest scoring label=0\n",
      "[ 0.1325 -0.      0.     -0.022  -0.     -0.0127 -0.0001 -0.0011 -0.\n",
      "  0.6754 -0.002  -0.0003 -0.0006 -0.     -0.1086 -0.     -0.0002]\n",
      "i=207, predicted_label=6, highest scoring label=11\n",
      "[-0.0006 -0.      0.     -0.3387 -0.     -0.0008  0.0846  0.     -0.0206\n",
      " -0.2139 -0.0001  0.783  -0.0001 -0.     -0.     -0.0001  0.    ]\n",
      "i=208, predicted_label=8, highest scoring label=7\n",
      "[-0.0061 -0.0007 -0.0397 -0.0002 -0.0047  0.0003 -0.0346 -0.001   0.9483\n",
      " -0.0492 -0.146  -0.2924 -0.0038 -0.0016 -0.0007 -0.     -0.0482]\n",
      "i=213, predicted_label=6, highest scoring label=12\n",
      "[-0.0002 -0.0053 -0.0004 -0.0035 -0.     -0.0008 -0.1735 -0.0021 -0.0082\n",
      " -0.0001 -0.0007 -0.164   0.716  -0.     -0.0053 -0.     -0.    ]\n",
      "i=219, predicted_label=10, highest scoring label=8\n",
      "[ 0.     -0.0006  0.0318 -0.0001 -0.0005 -0.0003 -0.0002 -0.0004 -0.0124\n",
      " -0.4957  0.9284 -0.24   -0.0032 -0.0003 -0.0006 -0.0931 -0.1239]\n",
      "i=222, predicted_label=8, highest scoring label=15\n",
      "[-0.0001  0.0026 -0.7658 -0.0114 -0.0023 -0.0001 -0.0007  0.0034  0.7744\n",
      "  0.08   -0.0014 -0.0175 -0.0004 -0.0001  0.     -0.128  -0.0039]\n",
      "i=223, predicted_label=8, highest scoring label=12\n",
      "[ 0.     -0.0187 -0.0229 -0.0078 -0.0001 -0.0004 -0.0007 -0.0001 -0.3595\n",
      "  0.0002 -0.0004  0.0005  0.8653 -0.0054 -0.084  -0.0591 -0.0004]\n",
      "i=245, predicted_label=10, highest scoring label=9\n",
      "[ 0.0001 -0.0034 -0.0145 -0.0003  0.     -0.2185 -0.0127 -0.0117 -0.3895\n",
      " -0.0641  0.4959 -0.0381 -0.0007 -0.     -0.0022 -0.0053 -0.0006]\n",
      "i=247, predicted_label=10, highest scoring label=7\n",
      "[-0.     -0.0051 -0.0004 -0.0079 -0.0022 -0.0005 -0.0001 -0.0034 -0.\n",
      " -0.0001  0.3125 -0.0002 -0.1112 -0.0001 -0.0009 -0.     -0.003 ]\n",
      "i=251, predicted_label=8, highest scoring label=7\n",
      "[-0.0001 -0.     -0.0054 -0.024  -0.     -0.0067 -0.0004  0.9031 -0.0002\n",
      " -0.0025 -0.     -0.0025 -0.0003 -0.0006 -0.0006 -0.1213 -0.0004]\n",
      "i=255, predicted_label=0, highest scoring label=7\n",
      "[ 0.0737 -0.0033 -0.0103 -0.0413  0.7535 -0.0001 -0.0003  0.0035 -0.0008\n",
      "  0.0694  0.0021 -0.0005 -0.0011 -0.0002 -0.0004 -0.003  -0.    ]\n",
      "i=257, predicted_label=15, highest scoring label=2\n",
      "[-0.0068 -0.0004  0.8995 -0.1199 -0.0078 -0.0001 -0.0007 -0.0056 -0.0017\n",
      " -0.0061 -0.0022 -0.0003 -0.0001 -0.0001 -0.0001 -0.0102 -0.0005]\n",
      "i=258, predicted_label=7, highest scoring label=9\n",
      "[-0.0002 -0.      0.974   0.0004  0.0012 -0.0022 -0.0001 -0.0105 -0.0009\n",
      " -0.0029 -0.003  -0.0107 -0.0709 -0.0018 -0.      0.0035 -0.0066]\n",
      "i=261, predicted_label=9, highest scoring label=15\n",
      "[-0.2366 -0.0003 -0.1386 -0.551  -0.0003 -0.0015  0.0004  0.1276  0.0035\n",
      " -0.1696 -0.0189 -0.0577 -0.0001 -0.     -0.0003  0.6601  0.0039]\n",
      "i=263, predicted_label=11, highest scoring label=3\n",
      "[-0.0473 -0.0063  0.0018  0.5039  0.      0.0009 -0.     -0.0002 -0.0177\n",
      " -0.0001  0.0477  0.0079  0.0074 -0.0256 -0.0001  0.2029  0.0008]\n",
      "i=268, predicted_label=1, highest scoring label=4\n",
      "[-0.0145 -0.0513 -0.0002 -0.4337  0.9998 -0.0002 -0.      0.0002 -0.0002\n",
      " -0.     -0.0003 -0.0003 -0.     -0.     -0.0002 -0.0005 -0.    ]\n",
      "i=270, predicted_label=14, highest scoring label=12\n",
      "[-0.0005 -0.1919 -0.0181 -0.0057 -0.      0.0004  0.0006 -0.0006  0.0152\n",
      " -0.0001 -0.014   0.0105  0.5419  0.0001  0.3611 -0.0017 -0.0227]\n"
     ]
    }
   ],
   "source": [
    "# check if predictions are matching but they are numpy arrays\n",
    "#assert np.array_equal(llama3_preds, get_predictions(all_evaluations, \"llama3\", \"shap-partition\"))\n",
    "\n",
    "for i, pred in enumerate(get_predictions(all_evaluations, \"unllama3\", \"attnlrp\")):\n",
    "    if unllama3_preds[i] != pred:\n",
    "        print(f\"i={i}, predicted_label={unllama3_preds[i]}, highest scoring label={pred}\")\n",
    "        print(all_evaluations[\"llama3\"][\"attnlrp\"][i, :, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average faithfulness scores ('scibert', 'attnlrp'): [ 0.5495 -0.0612  0.213 ]\n",
      "Average faithfulness scores ('scibert', 'shap-partition'): [ 0.3728 -0.0307  0.1229]\n",
      "Average faithfulness scores ('scibert', 'shap-partition-tfidf'): [ 0.4455 -0.0263  0.1381]\n",
      "Average faithfulness scores ('scibert', 'cplrp'): [0.481  0.0388 0.0623]\n",
      "Average faithfulness scores ('scibert', 'lime'): [ 0.0738  0.2321 -0.0066]\n",
      "Average faithfulness scores ('scibert', 'gradientxinput'): [ 0.1411  0.2402 -0.0237]\n",
      "Average faithfulness scores ('scibert', 'integrated-gradient'): [ 0.5181 -0.0154  0.102 ]\n",
      "\n",
      "Average faithfulness scores ('llama3', 'attnlrp'): [0.6321 0.0643 0.0821]\n",
      "Average faithfulness scores ('llama3', 'shap-partition'): [0.5489 0.1911 0.0233]\n",
      "Average faithfulness scores ('llama3', 'shap-partition-tfidf'): [0.4483 0.2698 0.0295]\n",
      "Average faithfulness scores ('llama3', 'cplrp'): [0.6013 0.0995 0.0404]\n",
      "Average faithfulness scores ('llama3', 'lime'): [ 0.019   0.257  -0.0029]\n",
      "Average faithfulness scores ('llama3', 'gradientxinput'): [ 0.2154  0.4596 -0.0016]\n",
      "Average faithfulness scores ('llama3', 'integrated-gradient'): [0.3057 0.376  0.0162]\n",
      "\n",
      "Average faithfulness scores ('unllama3', 'attnlrp'): [ 0.6016 -0.0711  0.0804]\n",
      "Average faithfulness scores ('unllama3', 'cplrp'): [ 0.5255 -0.0021  0.0358]\n",
      "Faithfulness eval against ground truth\n",
      "\n",
      "Average faithfulness scores ('scibert', 'attnlrp'): [ 0.4557 -0.1062  0.1955]\n",
      "Average faithfulness scores ('scibert', 'shap-partition'): [ 0.3103 -0.0667  0.1239]\n",
      "Average faithfulness scores ('scibert', 'shap-partition-tfidf'): [ 0.3736 -0.0678  0.1399]\n",
      "Average faithfulness scores ('scibert', 'cplrp'): [0.4047 0.0007 0.0586]\n",
      "Average faithfulness scores ('scibert', 'lime'): [ 0.0552  0.1795 -0.0067]\n",
      "Average faithfulness scores ('scibert', 'gradientxinput'): [ 0.1031  0.1819 -0.0202]\n",
      "Average faithfulness scores ('scibert', 'integrated-gradient'): [ 0.4344 -0.0465  0.0972]\n",
      "\n",
      "Average faithfulness scores ('llama3', 'attnlrp'): [ 0.5145 -0.0153  0.0779]\n",
      "Average faithfulness scores ('llama3', 'shap-partition'): [0.4467 0.0878 0.0256]\n",
      "Average faithfulness scores ('llama3', 'shap-partition-tfidf'): [0.3631 0.165  0.0315]\n",
      "Average faithfulness scores ('llama3', 'cplrp'): [0.4875 0.0254 0.0375]\n",
      "Average faithfulness scores ('llama3', 'lime'): [-0.0064  0.2142 -0.0036]\n",
      "Average faithfulness scores ('llama3', 'gradientxinput'): [ 0.1177  0.3533 -0.0035]\n",
      "Average faithfulness scores ('llama3', 'integrated-gradient'): [0.2111 0.2788 0.0156]\n",
      "\n",
      "Average faithfulness scores ('unllama3', 'attnlrp'): [ 0.5428 -0.0981  0.08  ]\n",
      "Average faithfulness scores ('unllama3', 'cplrp'): [ 0.4742 -0.0256  0.0309]\n"
     ]
    }
   ],
   "source": [
    "def get_average_scores(evaluations, predictions, model, method):\n",
    "    shap_evals = evaluations[model][method]\n",
    "\n",
    "    # Create a mask to select scores for predicted classes\n",
    "    mask = np.zeros_like(shap_evals, dtype=bool)\n",
    "    mask[np.arange(len(predictions)), predictions] = True\n",
    "\n",
    "    # Use the mask to select scores for predicted classes\n",
    "    selected_scores = shap_evals[mask].reshape(shap_evals.shape[0], shap_evals.shape[2])\n",
    "\n",
    "    # Calculate the average scores\n",
    "    average_scores = np.mean(selected_scores, axis=0)\n",
    "\n",
    "    return average_scores\n",
    "\n",
    "\n",
    "def print_scores(model, method, preds):\n",
    "    # Assuming all_evaluations is already loaded\n",
    "    average_scores = get_average_scores(all_evaluations, preds, model, method)\n",
    "    print(f\"Average faithfulness scores ('{model}', '{method}'): {average_scores}\")\n",
    "\n",
    "print_scores(\"scibert\", \"attnlrp\", scibert_preds)\n",
    "print_scores(\"scibert\", \"shap-partition\", scibert_preds)\n",
    "print_scores(\"scibert\", \"shap-partition-tfidf\", scibert_preds)\n",
    "print_scores(\"scibert\", \"cplrp\", scibert_preds)\n",
    "print_scores(\"scibert\", \"lime\", scibert_preds)\n",
    "print_scores(\"scibert\", \"gradientxinput\", scibert_preds)\n",
    "print_scores(\"scibert\", \"integrated-gradient\", scibert_preds)\n",
    "print()\n",
    "print_scores(\"llama3\", \"attnlrp\", llama3_preds)\n",
    "print_scores(\"llama3\", \"shap-partition\", llama3_preds)\n",
    "print_scores(\"llama3\", \"shap-partition-tfidf\", llama3_preds)\n",
    "print_scores(\"llama3\", \"cplrp\", llama3_preds)\n",
    "print_scores(\"llama3\", \"lime\", llama3_preds)\n",
    "print_scores(\"llama3\", \"gradientxinput\", llama3_preds)\n",
    "print_scores(\"llama3\", \"integrated-gradient\", llama3_preds)\n",
    "print()\n",
    "print_scores(\"unllama3\", \"attnlrp\", unllama3_preds)\n",
    "print_scores(\"unllama3\", \"cplrp\", unllama3_preds)\n",
    "\n",
    "print(\"Faithfulness eval against ground truth\")\n",
    "print()\n",
    "print_scores(\"scibert\", \"attnlrp\", ground_truth)\n",
    "print_scores(\"scibert\", \"shap-partition\", ground_truth)\n",
    "print_scores(\"scibert\", \"shap-partition-tfidf\", ground_truth)\n",
    "print_scores(\"scibert\", \"cplrp\", ground_truth)\n",
    "print_scores(\"scibert\", \"lime\", ground_truth)\n",
    "print_scores(\"scibert\", \"gradientxinput\", ground_truth)\n",
    "print_scores(\"scibert\", \"integrated-gradient\", ground_truth)\n",
    "print()\n",
    "print_scores(\"llama3\", \"attnlrp\", ground_truth)\n",
    "print_scores(\"llama3\", \"shap-partition\", ground_truth)\n",
    "print_scores(\"llama3\", \"shap-partition-tfidf\", ground_truth)\n",
    "print_scores(\"llama3\", \"cplrp\", ground_truth)\n",
    "print_scores(\"llama3\", \"lime\", ground_truth)\n",
    "print_scores(\"llama3\", \"gradientxinput\", ground_truth)\n",
    "print_scores(\"llama3\", \"integrated-gradient\", ground_truth)\n",
    "print()\n",
    "print_scores(\"unllama3\", \"attnlrp\", ground_truth)\n",
    "print_scores(\"unllama3\", \"cplrp\", ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12377475202083588"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_evaluations[\"llama3\"][\"attnlrp\"][36, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0003,  0.7505, -0.0005, -0.1261, -0.0006, -0.0007,  0.    ,\n",
       "        0.0001, -0.0003, -0.0003, -0.0923, -0.2396, -0.    , -0.006 ,\n",
       "       -0.0123, -0.0039, -0.0061])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_evaluations[\"unllama3\"][\"attnlrp\"][71][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness eval against predicted class and ground truth\n",
      "\n",
      "scibert-attnlrp & $0.550 \\pm 0.191$ & $-0.061 \\pm 0.127$ & $0.213 \\pm 0.126$ & $0.456 \\pm 0.290$ & $-0.106 \\pm 0.189$ & $0.195 \\pm 0.148$ \\\\\n",
      "scibert-shap-partition & $0.373 \\pm 0.187$ & $-0.031 \\pm 0.110$ & $0.123 \\pm 0.131$ & $0.310 \\pm 0.218$ & $-0.067 \\pm 0.154$ & $0.124 \\pm 0.133$ \\\\\n",
      "scibert-shap-partition-tfidf & $0.445 \\pm 0.226$ & $-0.026 \\pm 0.130$ & $0.138 \\pm 0.118$ & $0.374 \\pm 0.268$ & $-0.068 \\pm 0.177$ & $0.140 \\pm 0.115$ \\\\\n",
      "scibert-cplrp & $0.481 \\pm 0.241$ & $0.039 \\pm 0.134$ & $0.062 \\pm 0.084$ & $0.405 \\pm 0.296$ & $0.001 \\pm 0.166$ & $0.059 \\pm 0.087$ \\\\\n",
      "scibert-lime & $0.074 \\pm 0.149$ & $0.232 \\pm 0.203$ & $-0.007 \\pm 0.075$ & $0.055 \\pm 0.132$ & $0.180 \\pm 0.230$ & $-0.007 \\pm 0.073$ \\\\\n",
      "scibert-gradientxinput & $0.141 \\pm 0.226$ & $0.240 \\pm 0.248$ & $-0.024 \\pm 0.095$ & $0.103 \\pm 0.224$ & $0.182 \\pm 0.269$ & $-0.020 \\pm 0.094$ \\\\\n",
      "scibert-integrated-gradient & $0.518 \\pm 0.217$ & $-0.015 \\pm 0.120$ & $0.102 \\pm 0.099$ & $0.434 \\pm 0.282$ & $-0.046 \\pm 0.158$ & $0.097 \\pm 0.094$ \\\\\n",
      "\\hline\n",
      "llama3-attnlrp & $0.632 \\pm 0.262$ & $0.064 \\pm 0.200$ & $0.082 \\pm 0.101$ & $0.515 \\pm 0.353$ & $-0.015 \\pm 0.256$ & $0.078 \\pm 0.100$ \\\\\n",
      "llama3-shap-partition & $0.549 \\pm 0.289$ & $0.191 \\pm 0.288$ & $0.023 \\pm 0.089$ & $0.447 \\pm 0.356$ & $0.088 \\pm 0.344$ & $0.026 \\pm 0.089$ \\\\\n",
      "llama3-shap-partition-tfidf & $0.448 \\pm 0.336$ & $0.270 \\pm 0.304$ & $0.029 \\pm 0.080$ & $0.363 \\pm 0.363$ & $0.165 \\pm 0.354$ & $0.032 \\pm 0.084$ \\\\\n",
      "llama3-cplrp & $0.601 \\pm 0.289$ & $0.099 \\pm 0.204$ & $0.040 \\pm 0.101$ & $0.487 \\pm 0.370$ & $0.025 \\pm 0.244$ & $0.037 \\pm 0.099$ \\\\\n",
      "llama3-lime & $0.019 \\pm 0.286$ & $0.257 \\pm 0.422$ & $-0.003 \\pm 0.063$ & $-0.006 \\pm 0.284$ & $0.214 \\pm 0.419$ & $-0.004 \\pm 0.062$ \\\\\n",
      "llama3-gradientxinput & $0.215 \\pm 0.317$ & $0.460 \\pm 0.335$ & $-0.002 \\pm 0.075$ & $0.118 \\pm 0.337$ & $0.353 \\pm 0.380$ & $-0.003 \\pm 0.073$ \\\\\n",
      "llama3-integrated-gradient & $0.306 \\pm 0.322$ & $0.376 \\pm 0.306$ & $0.016 \\pm 0.068$ & $0.211 \\pm 0.342$ & $0.279 \\pm 0.346$ & $0.016 \\pm 0.067$ \\\\\n",
      "\\hline\n",
      "unllama3-attnlrp & $0.602 \\pm 0.339$ & $-0.071 \\pm 0.282$ & $0.080 \\pm 0.097$ & $0.543 \\pm 0.368$ & $-0.098 \\pm 0.319$ & $0.080 \\pm 0.097$ \\\\\n",
      "unllama3-cplrp & $0.526 \\pm 0.335$ & $-0.002 \\pm 0.250$ & $0.036 \\pm 0.095$ & $0.474 \\pm 0.361$ & $-0.026 \\pm 0.288$ & $0.031 \\pm 0.090$ \\\\\n",
      "\\hline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:26: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\p'\n",
      "/tmp/ipykernel_2003264/3677306318.py:26: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  latex_row += f\"${avg:.3f} \\pm {std:.3f}$ & \"\n",
      "/tmp/ipykernel_2003264/3677306318.py:28: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  latex_row += f\"${avg:.3f} \\pm {std:.3f}$ & \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_average_scores(evaluations, predictions, model, method):\n",
    "    shap_evals = evaluations[model][method]\n",
    "\n",
    "    # Create a mask to select scores for predicted classes\n",
    "    mask = np.zeros_like(shap_evals, dtype=bool)\n",
    "    mask[np.arange(len(predictions)), predictions] = True\n",
    "\n",
    "    # Use the mask to select scores for predicted classes\n",
    "    selected_scores = shap_evals[mask].reshape(shap_evals.shape[0], shap_evals.shape[2])\n",
    "\n",
    "    # Calculate the average scores and standard deviations\n",
    "    average_scores = np.mean(selected_scores, axis=0)\n",
    "    std_scores = np.std(selected_scores, axis=0)\n",
    "\n",
    "    return average_scores, std_scores\n",
    "\n",
    "def print_scores_latex(model, method, preds, ground_truth, use_ground_truth=True):\n",
    "    # Assuming all_evaluations is already loaded\n",
    "    pred_avg, pred_std = get_average_scores(all_evaluations, preds, model, method)\n",
    "    gt_avg, gt_std = get_average_scores(all_evaluations, ground_truth, model, method)\n",
    "\n",
    "    latex_row = f\"{model}-{method} & \"\n",
    "    for avg, std in zip(pred_avg, pred_std):\n",
    "        latex_row += f\"${avg:.3f} \\pm {std:.3f}$ & \"\n",
    "    if use_ground_truth:\n",
    "        for avg, std in zip(gt_avg, gt_std):\n",
    "            latex_row += f\"${avg:.3f} \\pm {std:.3f}$ & \"\n",
    "    latex_row = latex_row[:-2] + \"\\\\\\\\\"  # Remove last ' &' and add '\\\\'\n",
    "\n",
    "    print(latex_row)\n",
    "\n",
    "# Example usage\n",
    "print(\"Faithfulness eval against predicted class and ground truth\")\n",
    "print()\n",
    "models = [\"scibert\", \"llama3\", \"unllama3\"]\n",
    "methods = [\"attnlrp\", \"shap-partition\", \"shap-partition-tfidf\", \"cplrp\", \"lime\", \"gradientxinput\", \"integrated-gradient\"]\n",
    "\n",
    "for model in models:\n",
    "    for method in methods:\n",
    "        if model == \"unllama3\" and method not in [\"attnlrp\", \"cplrp\"]:\n",
    "            continue\n",
    "        preds = globals()[f\"{model}_preds\"]\n",
    "        print_scores_latex(model, method, preds, ground_truth, use_ground_truth=False)\n",
    "    print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc (faithfulness score measuring on viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "63 llama3 shappartition  'aopc_compr': 0.3317, 'aopc_suff': 0.0389, 'taucorr_loo': -0.0025\n",
    "\n",
    "63 scibert shappartition-tfidf 'aopc_compr': 0.7763, 'aopc_suff': 0.0856, 'taucorr_loo': 0.2726\n",
    "\n",
    "63 scibert shappartitioon: aopc_compr': 0.5178, 'aopc_suff': 0.0319, 'taucorr_loo': \n",
    "\n",
    "0.2622\n",
    "\n",
    "99 scibert shappartition aopc_compr': 0.6083, 'aopc_suff': 0.0659, 'taucorr_loo': -0.0073\n",
    "99 scibert shappartition tfidf 'aopc_compr': 0.6057, 'aopc_suff': -0.0191, 'taucorr_loo': 0.4045\n",
    "99 llama3 shappartition 'aopc_compr': 0.9825, 'aopc_suff': 0.0652, 'taucorr_loo': 0.0927\n",
    "99 llama3 shappartition tfidf {'aopc_compr': 0.9605, 'aopc_suff': 0.1943, 'taucorr_loo': 0.1256} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'aopc_compr': 0.0609, 'aopc_suff': 0.1632, 'taucorr_loo': -0.0595} method=cplrp model_family=scibert predicted_label=7 sample_index=63\n",
    "{'aopc_compr': 0.744, 'aopc_suff': 0.1166, 'taucorr_loo': 0.2232} method=attnlrp model_family=scibert predicted_label=7 sample_index=63\n",
    "{'aopc_compr': 0.7937, 'aopc_suff': 0.047, 'taucorr_loo': 0.1004} method=cplrp model_family=llama3 predicted_label=7 sample_index=63\n",
    "{'aopc_compr': 0.7755, 'aopc_suff': 0.0035, 'taucorr_loo': 0.0471} method=attnlrp model_family=llama3 predicted_label=7 sample_index=63\n",
    "{'aopc_compr': 0.675, 'aopc_suff': 0.1015, 'taucorr_loo': -0.1658} method=cplrp model_family=unllama3 predicted_label=7 sample_index=63\n",
    "{'aopc_compr': 0.7689, 'aopc_suff': 0.0228, 'taucorr_loo': 0.0944} method=attnlrp model_family=unllama3 predicted_label=7 sample_index=63\n",
    "\n",
    "{'aopc_compr': 0.6087, 'aopc_suff': 0.1188, 'taucorr_loo': 0.0475} method=cplrp model_family=scibert predicted_label=3 sample_index=99\n",
    "{'aopc_compr': 0.6143, 'aopc_suff': -0.007, 'taucorr_loo': 0.4057} method=attnlrp model_family=scibert predicted_label=3 sample_index=99\n",
    "{'aopc_compr': 0.9671, 'aopc_suff': 0.2611, 'taucorr_loo': 0.103} method=cplrp model_family=llama3 predicted_label=10 sample_index=99\n",
    "{'aopc_compr': 0.9123, 'aopc_suff': 0.1026, 'taucorr_loo': 0.1207} method=attnlrp model_family=llama3 predicted_label=10 sample_index=99\n",
    "{'aopc_compr': 0.994, 'aopc_suff': 0.105, 'taucorr_loo': 0.3772} method=cplrp model_family=unllama3 predicted_label=10 sample_index=99\n",
    "{'aopc_compr': 0.966, 'aopc_suff': 0.1284, 'taucorr_loo': 0.1804} method=attnlrp model_family=unllama3 predicted_label=10 sample_index=99\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
