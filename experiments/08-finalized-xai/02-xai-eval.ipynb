{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_evaluations(base_dir=\"evals\"):\n",
    "    evaluations = {}\n",
    "    for model in os.listdir(base_dir):\n",
    "        model_path = os.path.join(base_dir, model)\n",
    "        if os.path.isdir(model_path):\n",
    "            evaluations[model] = {}\n",
    "            for file in os.listdir(model_path):\n",
    "                if file.endswith('_evaluation.npy'):\n",
    "                    xai_method = file.split('_')[0]\n",
    "                    file_path = os.path.join(model_path, file)\n",
    "                    evaluations[model][xai_method] = np.load(file_path)\n",
    "    return evaluations\n",
    "\n",
    "# Load all evaluations\n",
    "all_evaluations = load_evaluations()\n",
    "\n",
    "# Compute averages and standard deviations\n",
    "def compute_stats(eval_array):\n",
    "    return np.mean(eval_array, axis=0), np.std(eval_array, axis=0)\n",
    "\n",
    "stats = {model: {xai: compute_stats(eval_array)\n",
    "                 for xai, eval_array in model_evals.items()}\n",
    "         for model, model_evals in all_evaluations.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity / correctness check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_nan(arr):\n",
    "    return np.isnan(arr).any()\n",
    "\n",
    "for model in all_evaluations:\n",
    "    for xai_method in all_evaluations[model]:\n",
    "        # Verify no score is NaN\n",
    "        if has_nan(all_evaluations[model][xai_method]):\n",
    "            print(f\"Model {model} with XAI method {xai_method} has NaNs\")\n",
    "        # check if all values are in the range [-1, 1]\n",
    "        if (all_evaluations[model][xai_method] > 1).any() or (all_evaluations[model][xai_method] < -1).any():\n",
    "            print(f\"Model {model} with XAI method {xai_method} has values outside of [-1, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271, 17, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_evaluations[\"scibert\"][\"shap-partition\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(evaluations, model, method):\n",
    "    shap_evals = evaluations[model][method]\n",
    "    first_scores = shap_evals[:, :, 0]\n",
    "    predictions = np.argmax(first_scores, axis=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_preds=[3,6,6,5,3,8,5,14,11,8,1,11,0,3,16,8,2,13,0,4,3,2,0,0,15,15,4,14,0,4,9,11,15,15,13,11,3,10,10,13,5,0,1,3,0,10,10,1,2,14,0,8,7,0,0,1,12,4,10,11,2,15,12,6,0,13,11,12,7,0,7,1,3,7,6,5,4,5,4,3,14,3,5,12,13,14,14,12,7,12,2,10,14,14,10,15,9,11,3,9,2,2,7,0,13,15,9,10,5,4,13,12,8,2,10,0,15,11,14,1,8,7,2,7,5,3,3,6,7,14,1,12,10,14,4,15,0,6,13,14,2,10,9,12,2,1,4,7,7,5,5,8,8,13,5,13,15,7,4,2,2,6,7,8,13,6,15,13,4,1,12,2,9,6,4,1,9,12,5,15,6,5,6,3,7,15,7,15,2,8,9,2,5,7,1,10,3,4,13,10,6,0,5,11,9,9,2,11,8,13,14,8,15,12,0,14,9,13,4,10,12,2,8,12,12,1,15,1,13,11,13,8,10,6,15,14,8,9,9,6,4,12,11,5,14,10,12,10,0,15,8,7,7,11,1,4,8,2,2,10,1,15,6,3,3,1,3,0,4,5,12]\n",
    "scibert_preds=[3,2,6,5,3,8,5,14,15,8,1,11,0,3,14,8,2,13,7,4,0,2,0,0,15,15,7,14,0,4,9,11,15,9,13,11,3,10,10,13,5,0,1,3,0,10,10,1,4,14,9,8,7,2,0,1,11,4,10,11,1,15,12,6,0,13,11,6,9,1,7,1,3,13,6,5,4,5,15,3,5,3,5,12,13,14,14,12,7,6,2,10,14,14,6,15,9,8,3,2,2,4,6,0,13,15,11,10,5,4,13,12,8,2,10,0,15,11,14,1,8,7,2,7,5,3,3,6,7,14,1,12,10,5,2,15,0,6,13,14,2,10,3,14,13,1,4,9,7,5,5,8,3,13,5,13,15,7,4,4,2,6,7,2,13,6,9,13,4,1,12,2,0,6,0,1,9,12,5,15,12,5,6,3,7,15,7,15,2,2,9,1,5,7,1,10,3,4,13,10,9,10,5,6,9,15,2,6,8,13,14,8,15,12,0,8,9,12,4,9,6,2,15,11,12,1,15,1,13,11,13,8,10,6,8,1,8,9,0,6,4,12,11,5,14,10,12,10,1,15,12,8,9,15,1,7,8,2,7,10,1,15,6,3,3,1,3,0,1,5,12]\n",
    "unllama3_preds=[3,2,6,5,3,8,5,14,14,8,4,11,0,3,14,8,2,13,0,4,3,2,0,0,15,15,2,12,0,4,9,11,15,9,13,11,3,10,10,13,5,0,1,3,0,10,10,1,15,14,0,8,7,2,0,1,11,4,10,11,2,15,12,6,0,13,11,6,9,0,7,1,3,14,6,5,4,5,4,3,12,3,5,12,13,14,14,12,2,12,2,10,14,14,10,15,9,11,3,9,2,9,7,0,13,15,11,10,5,4,13,12,8,2,6,0,15,11,14,1,11,3,2,7,5,3,3,6,7,14,1,12,10,12,4,15,0,6,13,14,2,10,9,14,2,1,4,7,7,5,5,8,8,13,5,13,15,9,4,5,2,11,7,2,13,6,11,13,4,1,12,15,9,6,4,1,7,12,5,15,12,5,6,3,7,15,7,15,2,2,9,2,5,7,1,10,3,4,13,10,8,0,5,11,9,4,2,6,8,13,14,8,15,6,0,8,9,13,4,10,12,2,8,8,12,1,9,1,13,11,13,8,10,6,15,1,8,9,0,6,4,12,11,5,14,10,12,10,1,15,8,8,7,15,1,0,8,15,7,10,1,9,6,11,3,1,3,0,1,5,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=8, predicted_label=14, highest scoring label=15\n",
      "[-0.0003 -0.0002 -0.1582 -0.162  -0.2492 -0.0034 -0.0004 -0.0012  0.0007\n",
      " -0.0042 -0.0674  0.2874 -0.0005 -0.2856 -0.0031  0.6037  0.0027]\n",
      "i=9, predicted_label=8, highest scoring label=7\n",
      "[-0.028  -0.0074 -0.0001 -0.0242 -0.0016 -0.001  -0.0004  0.0334  0.7483\n",
      "  0.0013  0.0018 -0.0007  0.0001 -0.0001 -0.0001 -0.0047 -0.    ]\n",
      "i=27, predicted_label=12, highest scoring label=14\n",
      "[-0.     -0.1235 -0.0042 -0.0018 -0.     -0.0025 -0.001  -0.0002 -0.0014\n",
      " -0.0007 -0.0317 -0.1342  0.5235 -0.0001  0.3686 -0.0001 -0.0001]\n",
      "i=33, predicted_label=9, highest scoring label=7\n",
      "[ 0.0001 -0.0046 -0.0001 -0.0443  0.0013 -0.     -0.0001  0.0685 -0.0021\n",
      " -0.0846 -0.0706 -0.0001  0.0001 -0.0009  0.      0.7859  0.0021]\n",
      "i=34, predicted_label=13, highest scoring label=5\n",
      "[-0.     -0.0001 -0.0165 -0.0001 -0.0002 -0.     -0.     -0.0006 -0.0272\n",
      " -0.     -0.0001 -0.0003 -0.      0.4322 -0.     -0.0037 -0.    ]\n",
      "i=38, predicted_label=10, highest scoring label=15\n",
      "[-0.0001 -0.     -0.0007 -0.2788 -0.     -0.003  -0.0001  0.0001 -0.001\n",
      " -0.      0.5682 -0.0005 -0.1098 -0.     -0.002   0.     -0.    ]\n",
      "i=48, predicted_label=15, highest scoring label=4\n",
      "[ 0.0002 -0.115   0.7865 -0.0891  0.0377 -0.0002 -0.0006  0.0901 -0.2252\n",
      " -0.0038 -0.     -0.0585 -0.0022 -0.0013 -0.002  -0.0028 -0.0005]\n",
      "i=51, predicted_label=8, highest scoring label=7\n",
      "[-0.0403 -0.1086 -0.0069 -0.0791 -0.0001 -0.0014  0.0034  0.1135  0.5\n",
      " -0.1438 -0.0006  0.0045  0.     -0.0029 -0.0025 -0.0036 -0.0035]\n",
      "i=52, predicted_label=7, highest scoring label=3\n",
      "[-0.0037 -0.0001 -0.0017 -0.5074 -0.0786 -0.0011 -0.0001  0.8355 -0.1348\n",
      " -0.0204 -0.0003 -0.     -0.     -0.0018 -0.0001 -0.0744 -0.    ]\n",
      "i=53, predicted_label=2, highest scoring label=0\n",
      "[ 0.7768 -0.0018 -0.0102 -0.0026 -0.     -0.0177 -0.001  -0.006  -0.0051\n",
      "  0.      0.      0.     -0.0001 -0.027  -0.     -0.0034 -0.0003]\n",
      "i=56, predicted_label=11, highest scoring label=12\n",
      "[-0.129  -0.0496 -0.1547  0.001  -0.0119 -0.0506 -0.2315 -0.3545 -0.1117\n",
      " -0.0183 -0.0082 -0.0007  0.7626 -0.0152  0.1121  0.0944 -0.3448]\n",
      "i=60, predicted_label=2, highest scoring label=0\n",
      "[ 0.0635  0.2587  0.5826 -0.0536 -0.0002 -0.0062 -0.2388 -0.0042 -0.0001\n",
      " -0.0689 -0.1193 -0.0242 -0.0417 -0.0005 -0.0027 -0.0015 -0.0019]\n",
      "i=62, predicted_label=12, highest scoring label=14\n",
      "[-0.     -0.0005 -0.0002 -0.0243 -0.0017 -0.0002 -0.0005 -0.     -0.0093\n",
      " -0.0014 -0.2329 -0.0102  0.8499 -0.0001  0.035  -0.     -0.0001]\n",
      "i=67, predicted_label=6, highest scoring label=12\n",
      "[-0.0108 -0.0003 -0.0009 -0.4113 -0.0018 -0.     -0.1467 -0.0046 -0.0226\n",
      " -0.0155 -0.0435 -0.0025  0.915  -0.     -0.0027 -0.0019 -0.0003]\n",
      "i=73, predicted_label=14, highest scoring label=7\n",
      "[-0.044  -0.3604 -0.2385 -0.2243 -0.0003 -0.7431 -0.0091  0.6838  0.\n",
      " -0.0004 -0.0806 -0.0002 -0.0011 -0.0087  0.2415 -0.0041  0.0021]\n",
      "i=75, predicted_label=5, highest scoring label=1\n",
      "[ 0.     -0.0036 -0.0033 -0.0026 -0.      0.2358 -0.      0.0003 -0.0005\n",
      " -0.0013  0.     -0.0075  0.     -0.0001 -0.     -0.     -0.0007]\n",
      "i=82, predicted_label=5, highest scoring label=15\n",
      "[ 0.0841  0.0002  0.2635  0.0199 -0.0069  0.5314  0.0004  0.0069 -0.0021\n",
      "  0.0149  0.0012 -0.0008  0.0005 -0.     -0.0001  0.0009  0.0026]\n",
      "i=96, predicted_label=9, highest scoring label=7\n",
      "[-0.2521 -0.1151 -0.093  -0.0456 -0.0063 -0.0004 -0.2459 -0.119  -0.\n",
      "  0.7827  0.     -0.     -0.     -0.0057 -0.0016 -0.1162 -0.0083]\n",
      "i=101, predicted_label=9, highest scoring label=4\n",
      "[-0.0041 -0.0862  0.562  -0.0028  0.0025 -0.0369 -0.0006 -0.0009 -0.0893\n",
      "  0.0014 -0.1169 -0.0006 -0.0028 -0.0001 -0.0002 -0.0105 -0.0039]\n",
      "i=106, predicted_label=11, highest scoring label=7\n",
      "[ 0.0005 -0.2247 -0.0006 -0.8072 -0.0376 -0.0052 -0.0334  0.25   -0.0085\n",
      "  0.3494 -0.0008  0.0162  0.0236 -0.0087  0.0003  0.0547 -0.0445]\n",
      "i=113, predicted_label=2, highest scoring label=3\n",
      "[-0.0001 -0.0129  0.3115  0.4303 -0.0005 -0.023  -0.0135 -0.0001 -0.0838\n",
      " -0.0326 -0.0256 -0.0066 -0.0005 -0.     -0.0015  0.0203 -0.0004]\n",
      "i=121, predicted_label=3, highest scoring label=15\n",
      "[ 0.0643 -0.008   0.0149  0.0303 -0.0097 -0.0028 -0.0054  0.6947 -0.0907\n",
      "  0.1189 -0.0291  0.0106 -0.0131 -0.017  -0.0013 -0.765  -0.0042]\n",
      "i=133, predicted_label=12, highest scoring label=5\n",
      "[-0.0253 -0.0042  0.     -0.0644 -0.     -0.0088 -0.0061 -0.0001 -0.0294\n",
      " -0.1094 -0.0004 -0.0002  0.0066 -0.0268  0.8829  0.     -0.0007]\n",
      "i=142, predicted_label=9, highest scoring label=3\n",
      "[ 0.0673 -0.0001 -0.0001 -0.     -0.0527 -0.0109 -0.      0.0001 -0.\n",
      "  0.6334  0.     -0.0001 -0.0001 -0.0023 -0.     -0.0003 -0.    ]\n",
      "i=143, predicted_label=14, highest scoring label=12\n",
      "[-0.0001  0.0044 -0.0008 -0.0246 -0.0023 -0.0087 -0.0074 -0.0384 -0.4029\n",
      " -0.0027 -0.0764 -0.003   0.5072 -0.0064  0.4196 -0.0015 -0.2845]\n",
      "i=147, predicted_label=7, highest scoring label=9\n",
      "[ 0.0019 -0.009  -0.0015 -0.0136  0.0019 -0.0003 -0.      0.8748 -0.2856\n",
      " -0.0797 -0.0009  0.0674 -0.     -0.     -0.0001 -0.122   0.0006]\n",
      "i=154, predicted_label=5, highest scoring label=10\n",
      "[-0.0002 -0.001  -0.0086 -0.0044 -0.0058  0.1131  0.3271  0.0004  0.0126\n",
      " -0.0008 -0.0057 -0.0134 -0.0891 -0.0035 -0.0068 -0.0066 -0.    ]\n",
      "i=163, predicted_label=2, highest scoring label=13\n",
      "[-0.008   0.0018 -0.0896 -0.0013 -0.0017 -0.1092 -0.1442 -0.0003  0.5271\n",
      " -0.1401  0.0886 -0.0323 -0.0243  0.1053 -0.0001  0.0006 -0.0108]\n",
      "i=166, predicted_label=11, highest scoring label=7\n",
      "[-0.2774 -0.0001  0.0012 -0.6648 -0.0056  0.     -0.003   0.0053  0.0014\n",
      " -0.4513 -0.0173  0.0086 -0.0003 -0.     -0.      0.9483  0.0121]\n",
      "i=185, predicted_label=15, highest scoring label=9\n",
      "[-0.0943 -0.1028  0.0075 -0.4336  0.089  -0.0379 -0.0002  0.2794 -0.0002\n",
      "  0.0434  0.0142  0.0002 -0.1409 -0.0254 -0.      0.3323 -0.0127]\n",
      "i=189, predicted_label=2, highest scoring label=8\n",
      "[-0.0008  0.4244  0.0281 -0.0147 -0.0003 -0.0052  0.0072 -0.0004  0.2944\n",
      " -0.0301  0.0047  0.0013 -0.0103 -0.0201 -0.     -0.0446 -0.0155]\n",
      "i=191, predicted_label=2, highest scoring label=1\n",
      "[-0.0089  0.0976  0.3423 -0.4945  0.0001 -0.0112  0.0008 -0.0006 -0.0135\n",
      " -0.1063 -0.0023 -0.0179 -0.0011 -0.0027  0.     -0.0004 -0.0001]\n",
      "i=197, predicted_label=4, highest scoring label=9\n",
      "[ 0.0029 -0.      0.      0.      0.867  -0.     -0.      0.0057 -0.\n",
      "  0.1241 -0.     -0.0001 -0.     -0.     -0.     -0.     -0.    ]\n",
      "i=200, predicted_label=8, highest scoring label=12\n",
      "[ 0.0028  0.0133 -0.0929 -0.1059 -0.0054 -0.0051  0.523   0.0597 -0.0078\n",
      " -0.3077 -0.003  -0.1389  0.3047 -0.      0.0244  0.0035 -0.0087]\n",
      "i=201, predicted_label=0, highest scoring label=10\n",
      "[ 0.5612  0.     -0.0003 -0.0004 -0.0108 -0.0004 -0.0001  0.0003 -0.0011\n",
      "  0.0003  0.2011  0.     -0.0228 -0.0001 -0.     -0.0207 -0.    ]\n",
      "i=204, predicted_label=9, highest scoring label=0\n",
      "[ 0.1325 -0.      0.     -0.022  -0.     -0.0127 -0.0001 -0.0011 -0.\n",
      "  0.6754 -0.002  -0.0003 -0.0006 -0.     -0.1086 -0.     -0.0002]\n",
      "i=207, predicted_label=6, highest scoring label=11\n",
      "[-0.0006 -0.      0.     -0.3387 -0.     -0.0008  0.0846  0.     -0.0206\n",
      " -0.2139 -0.0001  0.783  -0.0001 -0.     -0.     -0.0001  0.    ]\n",
      "i=208, predicted_label=8, highest scoring label=7\n",
      "[-0.0061 -0.0007 -0.0397 -0.0002 -0.0047  0.0003 -0.0346 -0.001   0.9483\n",
      " -0.0492 -0.146  -0.2924 -0.0038 -0.0016 -0.0007 -0.     -0.0482]\n",
      "i=213, predicted_label=6, highest scoring label=12\n",
      "[-0.0002 -0.0053 -0.0004 -0.0035 -0.     -0.0008 -0.1735 -0.0021 -0.0082\n",
      " -0.0001 -0.0007 -0.164   0.716  -0.     -0.0053 -0.     -0.    ]\n",
      "i=219, predicted_label=10, highest scoring label=8\n",
      "[ 0.     -0.0006  0.0318 -0.0001 -0.0005 -0.0003 -0.0002 -0.0004 -0.0124\n",
      " -0.4957  0.9284 -0.24   -0.0032 -0.0003 -0.0006 -0.0931 -0.1239]\n",
      "i=222, predicted_label=8, highest scoring label=15\n",
      "[-0.0001  0.0026 -0.7658 -0.0114 -0.0023 -0.0001 -0.0007  0.0034  0.7744\n",
      "  0.08   -0.0014 -0.0175 -0.0004 -0.0001  0.     -0.128  -0.0039]\n",
      "i=223, predicted_label=8, highest scoring label=12\n",
      "[ 0.     -0.0187 -0.0229 -0.0078 -0.0001 -0.0004 -0.0007 -0.0001 -0.3595\n",
      "  0.0002 -0.0004  0.0005  0.8653 -0.0054 -0.084  -0.0591 -0.0004]\n",
      "i=245, predicted_label=10, highest scoring label=9\n",
      "[ 0.0001 -0.0034 -0.0145 -0.0003  0.     -0.2185 -0.0127 -0.0117 -0.3895\n",
      " -0.0641  0.4959 -0.0381 -0.0007 -0.     -0.0022 -0.0053 -0.0006]\n",
      "i=247, predicted_label=10, highest scoring label=7\n",
      "[-0.     -0.0051 -0.0004 -0.0079 -0.0022 -0.0005 -0.0001 -0.0034 -0.\n",
      " -0.0001  0.3125 -0.0002 -0.1112 -0.0001 -0.0009 -0.     -0.003 ]\n",
      "i=251, predicted_label=8, highest scoring label=7\n",
      "[-0.0001 -0.     -0.0054 -0.024  -0.     -0.0067 -0.0004  0.9031 -0.0002\n",
      " -0.0025 -0.     -0.0025 -0.0003 -0.0006 -0.0006 -0.1213 -0.0004]\n",
      "i=255, predicted_label=0, highest scoring label=7\n",
      "[ 0.0737 -0.0033 -0.0103 -0.0413  0.7535 -0.0001 -0.0003  0.0035 -0.0008\n",
      "  0.0694  0.0021 -0.0005 -0.0011 -0.0002 -0.0004 -0.003  -0.    ]\n",
      "i=257, predicted_label=15, highest scoring label=2\n",
      "[-0.0068 -0.0004  0.8995 -0.1199 -0.0078 -0.0001 -0.0007 -0.0056 -0.0017\n",
      " -0.0061 -0.0022 -0.0003 -0.0001 -0.0001 -0.0001 -0.0102 -0.0005]\n",
      "i=258, predicted_label=7, highest scoring label=9\n",
      "[-0.0002 -0.      0.974   0.0004  0.0012 -0.0022 -0.0001 -0.0105 -0.0009\n",
      " -0.0029 -0.003  -0.0107 -0.0709 -0.0018 -0.      0.0035 -0.0066]\n",
      "i=261, predicted_label=9, highest scoring label=15\n",
      "[-0.2366 -0.0003 -0.1386 -0.551  -0.0003 -0.0015  0.0004  0.1276  0.0035\n",
      " -0.1696 -0.0189 -0.0577 -0.0001 -0.     -0.0003  0.6601  0.0039]\n",
      "i=263, predicted_label=11, highest scoring label=3\n",
      "[-0.0473 -0.0063  0.0018  0.5039  0.      0.0009 -0.     -0.0002 -0.0177\n",
      " -0.0001  0.0477  0.0079  0.0074 -0.0256 -0.0001  0.2029  0.0008]\n",
      "i=268, predicted_label=1, highest scoring label=4\n",
      "[-0.0145 -0.0513 -0.0002 -0.4337  0.9998 -0.0002 -0.      0.0002 -0.0002\n",
      " -0.     -0.0003 -0.0003 -0.     -0.     -0.0002 -0.0005 -0.    ]\n",
      "i=270, predicted_label=14, highest scoring label=12\n",
      "[-0.0005 -0.1919 -0.0181 -0.0057 -0.      0.0004  0.0006 -0.0006  0.0152\n",
      " -0.0001 -0.014   0.0105  0.5419  0.0001  0.3611 -0.0017 -0.0227]\n"
     ]
    }
   ],
   "source": [
    "# check if predictions are matching but they are numpy arrays\n",
    "#assert np.array_equal(llama3_preds, get_predictions(all_evaluations, \"llama3\", \"shap-partition\"))\n",
    "\n",
    "for i, pred in enumerate(get_predictions(all_evaluations, \"unllama3\", \"attnlrp\")):\n",
    "    if unllama3_preds[i] != pred:\n",
    "        print(f\"i={i}, predicted_label={unllama3_preds[i]}, highest scoring label={pred}\")\n",
    "        print(all_evaluations[\"llama3\"][\"attnlrp\"][i, :, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average faithfulness scores ('scibert', 'attnlrp'): [ 0.5495 -0.0612  0.213 ]\n",
      "Average faithfulness scores ('scibert', 'shap-partition'): [ 0.3728 -0.0307  0.1229]\n",
      "Average faithfulness scores ('scibert', 'shap-partition-tfidf'): [ 0.4455 -0.0263  0.1381]\n",
      "Average faithfulness scores ('scibert', 'cplrp'): [0.481  0.0388 0.0623]\n",
      "Average faithfulness scores ('scibert', 'lime'): [ 0.0738  0.2321 -0.0066]\n",
      "Average faithfulness scores ('scibert', 'gradientxinput'): [ 0.1411  0.2402 -0.0237]\n",
      "Average faithfulness scores ('scibert', 'integrated-gradient'): [ 0.5181 -0.0154  0.102 ]\n",
      "\n",
      "Average faithfulness scores ('llama3', 'attnlrp'): [0.6321 0.0643 0.0821]\n",
      "Average faithfulness scores ('llama3', 'shap-partition'): [0.5489 0.1911 0.0233]\n",
      "Average faithfulness scores ('llama3', 'shap-partition-tfidf'): [0.4483 0.2698 0.0295]\n",
      "Average faithfulness scores ('llama3', 'cplrp'): [0.6013 0.0995 0.0404]\n",
      "Average faithfulness scores ('llama3', 'lime'): [ 0.019   0.257  -0.0029]\n",
      "Average faithfulness scores ('llama3', 'gradientxinput'): [ 0.2154  0.4596 -0.0016]\n",
      "Average faithfulness scores ('llama3', 'integrated-gradient'): [0.3057 0.376  0.0162]\n",
      "\n",
      "Average faithfulness scores ('unllama3', 'attnlrp'): [ 0.6016 -0.0711  0.0804]\n"
     ]
    }
   ],
   "source": [
    "def get_average_scores(evaluations, predictions, model, method):\n",
    "    shap_evals = evaluations[model][method]\n",
    "\n",
    "    # Create a mask to select scores for predicted classes\n",
    "    mask = np.zeros_like(shap_evals, dtype=bool)\n",
    "    mask[np.arange(len(predictions)), predictions] = True\n",
    "\n",
    "    # Use the mask to select scores for predicted classes\n",
    "    selected_scores = shap_evals[mask].reshape(shap_evals.shape[0], shap_evals.shape[2])\n",
    "\n",
    "    # Calculate the average scores\n",
    "    average_scores = np.mean(selected_scores, axis=0)\n",
    "\n",
    "    return average_scores\n",
    "\n",
    "\n",
    "def print_scores(model, method, preds):\n",
    "    # Assuming all_evaluations is already loaded\n",
    "    average_scores = get_average_scores(all_evaluations, preds, model, method)\n",
    "    print(f\"Average faithfulness scores ('{model}', '{method}'): {average_scores}\")\n",
    "\n",
    "print_scores(\"scibert\", \"attnlrp\", scibert_preds)\n",
    "print_scores(\"scibert\", \"shap-partition\", scibert_preds)\n",
    "print_scores(\"scibert\", \"shap-partition-tfidf\", scibert_preds)\n",
    "print_scores(\"scibert\", \"cplrp\", scibert_preds)\n",
    "print_scores(\"scibert\", \"lime\", scibert_preds)\n",
    "print_scores(\"scibert\", \"gradientxinput\", scibert_preds)\n",
    "print_scores(\"scibert\", \"integrated-gradient\", scibert_preds)\n",
    "print()\n",
    "print_scores(\"llama3\", \"attnlrp\", llama3_preds)\n",
    "print_scores(\"llama3\", \"shap-partition\", llama3_preds)\n",
    "print_scores(\"llama3\", \"shap-partition-tfidf\", llama3_preds)\n",
    "print_scores(\"llama3\", \"cplrp\", llama3_preds)\n",
    "print_scores(\"llama3\", \"lime\", llama3_preds)\n",
    "print_scores(\"llama3\", \"gradientxinput\", llama3_preds)\n",
    "print_scores(\"llama3\", \"integrated-gradient\", llama3_preds)\n",
    "print()\n",
    "print_scores(\"unllama3\", \"attnlrp\", unllama3_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12377475202083588"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_evaluations[\"llama3\"][\"attnlrp\"][36, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0003,  0.7505, -0.0005, -0.1261, -0.0006, -0.0007,  0.    ,\n",
       "        0.0001, -0.0003, -0.0003, -0.0923, -0.2396, -0.    , -0.006 ,\n",
       "       -0.0123, -0.0039, -0.0061])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_evaluations[\"unllama3\"][\"attnlrp\"][71][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
