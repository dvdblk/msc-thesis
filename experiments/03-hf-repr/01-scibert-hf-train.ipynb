{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCIBERT in HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this script to import the variables and settings from ipynb_util.py\n",
    "%run ../ipynb_util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select dataset\n",
    "from enum import Enum\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    \"\"\"Enum for the dataset type.\"\"\"\n",
    "\n",
    "    \"\"\"Zora + OSDG upsampled dataset\"\"\"\n",
    "    ZO_UP = \"zo_up\"\n",
    "    \"\"\"SwissText Shared Task 1 dataset (Zurich NLP)\"\"\"\n",
    "    SWISSTEXT_SHARED_TASK1 = \"swisstext_shared_task1\"\n",
    "\n",
    "\n",
    "DATASET_TYPE = DatasetType.ZO_UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SDG': ClassLabel(names=['1', '10', '11', '12', '13', '14', '15', '16', '17', '2', '3', '4', '5', '6', '7', '8', '9'], id=None), 'ABSTRACT': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'sdg_desc_short': Value(dtype='string', id=None), 'sdg_desc_long': Value(dtype='string', id=None)}\n",
      "Example instance:\t {'SDG': 16, 'ABSTRACT': 'The first attempts to modernize simply replaced the single huge engine with a huge electric motor, changing little. The drive-shafts were replaced by wires, the huge steam engine by dozens of small motors. Factories spread out, there was natural light, and room to use ceiling-slung cranes. Workers had responsibility for their own machines, they needed better training and better pay. The electric motor was a wonderful invention, once we changed all the everyday details that surrounded it.', 'id': None, 'sdg_desc_short': None, 'sdg_desc_long': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Features, Value, ClassLabel\n",
    "import pickle\n",
    "\n",
    "# load the dataset\n",
    "# note: if you don't have the data in the folder, use the download-data.sh script\n",
    "\n",
    "match DATASET_TYPE:\n",
    "    case DatasetType.ZO_UP:\n",
    "        # dont need to use manual features as class_encode_column will create ClassLabel\n",
    "        # careful: watch out for the order of the ClassLabel as it doesn't map directly to the SDG class. need use mapping functions (id2label, label2id)\n",
    "        # sdgs = [str(i) for i in range(1, 18)] + [\"non-relevant\"]\n",
    "        # features = Features({\"sdg\": ClassLabel(num_classes=len(sdgs), names=sdgs), \"abstract\": Value(\"string\")})\n",
    "\n",
    "        dataset = load_dataset(\"csv\", data_files=str(DATA_DIR_PATH / \"zo_up.csv\"))\n",
    "        dataset = dataset.rename_columns({\"sdg\": \"SDG\", \"abstract\": \"ABSTRACT\"}).class_encode_column(\"SDG\")\n",
    "        dataset = dataset[\"train\"].train_test_split(test_size=0.3, stratify_by_column=\"SDG\", seed=SEED)\n",
    "    case DatasetType.SWISSTEXT_SHARED_TASK1:\n",
    "        dataset = load_dataset(\"json\", data_files=str(DATA_DIR_PATH / \"swisstext-2024-sharedtask\" / \"task1-train.jsonl\"))\n",
    "        dataset = dataset[\"train\"].train_test_split(test_size=0.3, seed=SEED)\n",
    "\n",
    "print(dataset[\"train\"].features)\n",
    "example = dataset[\"train\"][0]\n",
    "print(\"Example instance:\\t\", example)\n",
    "\n",
    "\n",
    "# Label encodings / mappings\n",
    "labels = set(dataset[\"train\"][\"SDG\"])\n",
    "id2label = {i: dataset[\"train\"].features[\"SDG\"].int2str(i) for i in range(len(labels))}\n",
    "label2id = {dataset[\"train\"].features[\"SDG\"].int2str(i): i for i in range(len(labels))}\n",
    "\n",
    "# save the encodings to a file for later use\n",
    "ENCODING_DIR = BASE_DIR_PATH / \"encodings\" / DATASET_TYPE.value\n",
    "# create the directory if it doesn't exist\n",
    "ENCODING_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(ENCODING_DIR / \"id2label.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id2label, f)\n",
    "\n",
    "with open(ENCODING_DIR / \"label2id.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label2id, f)\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded (label2id) label:\t 16\n",
      "Decoded (id2label) label:\t 9\n",
      "9 16 16\n"
     ]
    }
   ],
   "source": [
    "# verify that encodings work properly on example instance\n",
    "# example instance has label 9 in the csv file\n",
    "# example instance has label 16 in the encoded dataset\n",
    "assert example[\"SDG\"] == label2id[id2label[example[\"SDG\"]]]\n",
    "print(\"Encoded (label2id) label:\\t\", example[\"SDG\"])\n",
    "print(\"Decoded (id2label) label:\\t\", id2label[example[\"SDG\"]])\n",
    "\n",
    "print(id2label[16], label2id[id2label[16]], label2id[\"9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <enum 'DatasetType'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <enum 'DatasetType'>: __main__.DatasetType has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe125ce20924723bcfc21b4381dc653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e07c6a6783a49cb83118d516e5946dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/271 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# whether the text should be lowered or not\n",
    "SHOULD_LOWER = True\n",
    "\n",
    "# base model\n",
    "HF_MODEL_NAME = \"allenai/scibert_scivocab_uncased\"\n",
    "# final model\n",
    "MODEL_NAME = f\"{HF_MODEL_NAME}-ft-{DATASET_TYPE.value}\" + (\"-lower\" if SHOULD_LOWER else \"\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
    "\n",
    "def preprocess_data(instances):\n",
    "    match DATASET_TYPE:\n",
    "        case DatasetType.SWISSTEXT_SHARED_TASK1:\n",
    "            # take a batch of titles and abstracts and concat them\n",
    "            titles = instances[\"TITLE\"]\n",
    "            abstracts = instances[\"ABSTRACT\"]\n",
    "            texts = [f\"{title} {abstract}\" for title, abstract in zip(titles, abstracts)]\n",
    "        case DatasetType.ZO_UP:\n",
    "            texts = instances[\"ABSTRACT\"]\n",
    "\n",
    "    if SHOULD_LOWER:\n",
    "        texts = [text.lower() for text in texts]\n",
    "\n",
    "    # encode\n",
    "    encoding = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # add labels\n",
    "    encoding[\"label\"] = torch.tensor([label for label in instances[\"SDG\"]])\n",
    "\n",
    "    return encoding\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example instance:\t {'input_ids': tensor([  102,   111,   705,  7834,   147,  5901,   767,  4427,  6703,   111,\n",
      "         1232, 11812,  2393,   190,   106, 11812,  5612,  3850,   422,  5468,\n",
      "         3441,   205,   111,  7021,   579, 20665, 30113,   267,  6703,   214,\n",
      "        17162,   422,   111, 11812, 22668,  2393,   214,   572, 11451,   131,\n",
      "          952, 17898,   205,  1491,   301,  4696,   556,   422,   461,   241,\n",
      "         2404,  2011,   422,   137,  4095,   147,   626, 26503,   579,  1252,\n",
      "          794, 21830,  6820,   205,  5555,   883,  9945,   168,   547,  2910,\n",
      "         7909,   422,   698,  2764,  1883,  2208,   137,  1883,  3982,   205,\n",
      "          111,  5612,  3850,   241,   106, 23398,  1004, 28364,   422,  3246,\n",
      "          185,  5414,   355,   111, 15304,  3779,   198, 17771,   256,   205,\n",
      "          103,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(16)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] the first attempts to modernize simply replaced the single huge engine with a huge electric motor, changing little. the drive - shafts were replaced by wires, the huge steam engine by dozens of small motors. factories spread out, there was natural light, and room to use ceiling - slung cranes. workers had responsibility for their own machines, they needed better training and better pay. the electric motor was a wonderful invention, once we changed all the everyday details that surrounded it. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = encoded_dataset[\"train\"][0]\n",
    "print(\"Example instance:\\t\", example)\n",
    "\n",
    "tokenizer.decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    HF_MODEL_NAME,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "METRIC_NAME = \"accuracy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{CHECKPOINT_PATH}/{MODEL_NAME}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    #save_steps=10,\n",
    "    #eval_steps=10,\n",
    "    #logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=15,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=METRIC_NAME,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "def compute_metrics(pred: EvalPrediction):\n",
    "    labels = pred.label_ids\n",
    "    accuracy = accuracy_score(labels, pred.predictions.argmax(-1))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred.predictions.argmax(-1), average=\"weighted\")\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1185' max='1185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1185/1185 15:41, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.543400</td>\n",
       "      <td>1.996294</td>\n",
       "      <td>0.568266</td>\n",
       "      <td>0.573122</td>\n",
       "      <td>0.568266</td>\n",
       "      <td>0.531218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.415400</td>\n",
       "      <td>1.264565</td>\n",
       "      <td>0.678967</td>\n",
       "      <td>0.669639</td>\n",
       "      <td>0.678967</td>\n",
       "      <td>0.664313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.716700</td>\n",
       "      <td>1.086663</td>\n",
       "      <td>0.708487</td>\n",
       "      <td>0.698641</td>\n",
       "      <td>0.708487</td>\n",
       "      <td>0.695318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.387600</td>\n",
       "      <td>1.054023</td>\n",
       "      <td>0.719557</td>\n",
       "      <td>0.716750</td>\n",
       "      <td>0.719557</td>\n",
       "      <td>0.711743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>1.205441</td>\n",
       "      <td>0.726937</td>\n",
       "      <td>0.730698</td>\n",
       "      <td>0.726937</td>\n",
       "      <td>0.718986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>1.278205</td>\n",
       "      <td>0.730627</td>\n",
       "      <td>0.728600</td>\n",
       "      <td>0.730627</td>\n",
       "      <td>0.724242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>1.399384</td>\n",
       "      <td>0.704797</td>\n",
       "      <td>0.705396</td>\n",
       "      <td>0.704797</td>\n",
       "      <td>0.698122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>1.392856</td>\n",
       "      <td>0.701107</td>\n",
       "      <td>0.692765</td>\n",
       "      <td>0.701107</td>\n",
       "      <td>0.692113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>1.493152</td>\n",
       "      <td>0.708487</td>\n",
       "      <td>0.702716</td>\n",
       "      <td>0.708487</td>\n",
       "      <td>0.699448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>1.530026</td>\n",
       "      <td>0.712177</td>\n",
       "      <td>0.713591</td>\n",
       "      <td>0.712177</td>\n",
       "      <td>0.705070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>1.519035</td>\n",
       "      <td>0.704797</td>\n",
       "      <td>0.699055</td>\n",
       "      <td>0.704797</td>\n",
       "      <td>0.696234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>1.563341</td>\n",
       "      <td>0.708487</td>\n",
       "      <td>0.707866</td>\n",
       "      <td>0.708487</td>\n",
       "      <td>0.701813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>1.570230</td>\n",
       "      <td>0.704797</td>\n",
       "      <td>0.699055</td>\n",
       "      <td>0.704797</td>\n",
       "      <td>0.696234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>1.576078</td>\n",
       "      <td>0.704797</td>\n",
       "      <td>0.699055</td>\n",
       "      <td>0.704797</td>\n",
       "      <td>0.696234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>1.582343</td>\n",
       "      <td>0.708487</td>\n",
       "      <td>0.704183</td>\n",
       "      <td>0.708487</td>\n",
       "      <td>0.701126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1185, training_loss=0.37262449898297273, metrics={'train_runtime': 942.6527, 'train_samples_per_second': 10.025, 'train_steps_per_second': 1.257, 'total_flos': 2486734338816000.0, 'train_loss': 0.37262449898297273, 'epoch': 15.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF eval {'eval_loss': 1.2782045602798462, 'eval_accuracy': 0.7306273062730627, 'eval_precision': 0.728599502232732, 'eval_recall': 0.7306273062730627, 'eval_f1': 0.7242417970927734, 'eval_runtime': 9.2309, 'eval_samples_per_second': 29.358, 'eval_steps_per_second': 3.683, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# manual evaluation to show classifcation_report\n",
    "true_labels = []\n",
    "logits = []\n",
    "\n",
    "for batch in encoded_dataset[\"test\"]:\n",
    "    batch = {k: v.to(trainer.args.device).unsqueeze(0) for k, v in batch.items()}\n",
    "    label = batch.pop(\"label\")\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        out = model(**batch)\n",
    "\n",
    "    true_labels.append(label.item())\n",
    "    logits.extend(out.logits.tolist())\n",
    "\n",
    "probabilites = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
    "pred_labels = torch.argmax(probabilites, dim=-1).tolist()\n",
    "\n",
    "report = classification_report(true_labels, pred_labels, target_names=[f\"SDG {id2label[i]}\" for i in range(len(labels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       SDG 1       0.78      0.82      0.80        17\n",
      "      SDG 10       0.42      0.47      0.44        17\n",
      "      SDG 11       0.80      0.71      0.75        17\n",
      "      SDG 12       0.57      0.47      0.52        17\n",
      "      SDG 13       0.72      0.76      0.74        17\n",
      "      SDG 14       1.00      0.94      0.97        17\n",
      "      SDG 15       0.80      0.94      0.86        17\n",
      "      SDG 16       0.69      0.65      0.67        17\n",
      "      SDG 17       0.00      0.00      0.00         1\n",
      "       SDG 2       0.63      0.75      0.69        16\n",
      "       SDG 3       0.71      0.88      0.79        17\n",
      "       SDG 4       0.82      0.82      0.82        17\n",
      "       SDG 5       0.76      0.94      0.84        17\n",
      "       SDG 6       0.79      0.88      0.83        17\n",
      "       SDG 7       0.80      0.71      0.75        17\n",
      "       SDG 8       0.70      0.44      0.54        16\n",
      "       SDG 9       0.69      0.53      0.60        17\n",
      "\n",
      "    accuracy                           0.73       271\n",
      "   macro avg       0.69      0.69      0.68       271\n",
      "weighted avg       0.73      0.73      0.72       271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "text = \"Priming of microbial microcystin degradation in biomass-fed gravity driven membrane filtration biofilms Gravity-driven membrane (GDM) filtration is a promising tool for low-cost decentralized drinking water production. The biofilms in GDM systems are able of removing harmful chemical components, particularly toxic cyanobacterial metabolites such as microcystins (MCs). This is relevant for the application of GDM filtration because anthropogenic nutrient input and climate change have led to an increase of toxic cyanobacterial blooms. However, removal of MCs in newly developing GDM biofilms is only established after a prolonged period of time. Since cyanobacterial blooms are transient phenomena, it is important to understand MC removal in mature biofilms with or without prior toxin exposure. In this study, the microbial community composition of GDM biofilms was investigated in systems fed with water from a lake with periodic blooms of MC-producing cyanobacteria. Two out of three experimental treatments were supplemented with dead biomass of a MC-containing cyanobacterial strain, or of a non-toxic mutant, respectively. Analysis of bacterial rRNA genes revealed that both biomass-amended treatments were significantly more similar to each other than to a non-supplemented control. Therefore, it was hypothesized that biofilms could potentially be 'primed' for rapid MC removal by prior addition of non-toxic biomass. A subsequent experiment showed that MC removal developed significantly faster in mature biofilms that were pre-fed with biomass from the mutant strain than in unamended controls, indicating that MC degradation was a facultative trait of bacterial populations in GDM biofilms. The significant enrichment of bacteria related to both aerobic and anaerobic MC degraders suggested that this process might have occurred in parallel in different microniches.\"\n",
    "\n",
    "MODEL_PATH = f\"{CHECKPOINT_PATH}/{MODEL_NAME}/checkpoint-474/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model.eval()\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(model.device) for k, v in encoding.items()}\n",
    "\n",
    "outputs = model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0043, 0.0046, 0.0038, 0.0069, 0.0203, 0.4612, 0.0245, 0.0032, 0.0041,\n",
      "        0.0053, 0.0047, 0.0035, 0.0031, 0.4366, 0.0068, 0.0024, 0.0048],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax()\n",
    "probs = softmax(logits.squeeze().cpu())\n",
    "\n",
    "# for multi-label classification, we need to threshold the probabilities\n",
    "\"\"\"\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "predicted_labels = [id2label(i) for i in np.where(predictions == 1)]\n",
    "print(predicted_labels)\n",
    "\"\"\"\n",
    "# for single-label classification, we can take the argmax\n",
    "print(probs)\n",
    "predictions = probs.argmax(-1)\n",
    "predicted_label = id2label[predictions.int().item()]\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def my_reset(*varnames):\n",
    "    \"\"\"\n",
    "    Resets global variables and majority of CUDA memory. Only works in Jupyter.\n",
    "\n",
    "    varnames are what you want to keep\n",
    "    \"\"\"\n",
    "    try:\n",
    "        del model\n",
    "    except NameError:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    globals_ = globals()\n",
    "    to_save = {v: globals_[v] for v in varnames}\n",
    "    to_save['my_reset'] = my_reset  # lets keep this function by default\n",
    "    del globals_\n",
    "    get_ipython().magic(\"reset\")\n",
    "    globals().update(to_save)\n",
    "\n",
    "# my_reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating sklearn precision warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9873459873459873, 0.1282051282051282, 0.12816755893678972, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.ones((86, 18)).argmax(-1)\n",
    "p[:10] = torch.Tensor([i for i in range(10)])\n",
    "t = torch.ones(86, dtype=torch.long)\n",
    "t[:18] = torch.Tensor([i for i in range(18)])\n",
    "\n",
    "# no Warning\n",
    "precision_recall_fscore_support(t, p, average=\"weighted\", labels=[i for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvdblk/dev/_thesis/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9748479368732533, 0.12658227848101267, 0.12654518477303286, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warning\n",
    "precision_recall_fscore_support(t, p, average=\"weighted\", labels=[i for i in range(11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
