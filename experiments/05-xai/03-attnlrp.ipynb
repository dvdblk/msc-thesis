{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ipynb_util_tars.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SDG': ClassLabel(names=['1', '10', '11', '12', '13', '14', '15', '16', '17', '2', '3', '4', '5', '6', '7', '8', '9'], id=None), 'ABSTRACT': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'sdg_desc_short': Value(dtype='string', id=None), 'sdg_desc_long': Value(dtype='string', id=None)}\n",
      "Example instance:\t {'SDG': 16, 'ABSTRACT': 'The first attempts to modernize simply replaced the single huge engine with a huge electric motor, changing little. The drive-shafts were replaced by wires, the huge steam engine by dozens of small motors. Factories spread out, there was natural light, and room to use ceiling-slung cranes. Workers had responsibility for their own machines, they needed better training and better pay. The electric motor was a wonderful invention, once we changed all the everyday details that surrounded it.', 'id': None, 'sdg_desc_short': None, 'sdg_desc_long': None}\n",
      "Encoded (label2id) label:\t 16\n",
      "Decoded (id2label) label:\t 9\n",
      "9 16 16\n"
     ]
    }
   ],
   "source": [
    "%run ../ipynb_load_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=6, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== \n",
      "\n",
      "AttnLRP llama rules: LlamaForSequenceClassification(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048, padding_idx=2)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): LinearEpsilon(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): LinearEpsilon(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): LinearEpsilon(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): LinearEpsilon(in_features=2048, out_features=2048, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (attn_value_matmul): AttentionValueMatmul()\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): LinearEpsilon(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): LinearEpsilon(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): LinearEpsilon(in_features=5632, out_features=2048, bias=False)\n",
      "          (silu): SiLU()\n",
      "          (proj_silu_mul): ProjSiluMultiplication()\n",
      "        )\n",
      "        (input_layernorm): RMSNormIdentity()\n",
      "        (post_attention_layernorm): RMSNormIdentity()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNormIdentity()\n",
      "  )\n",
      "  (score): LinearEpsilon(in_features=2048, out_features=17, bias=False)\n",
      ")\n",
      "======================================== \n",
      "\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping q_proj because it is already a LinearEpsilon\n",
      "Skipping k_proj because it is already a LinearEpsilon\n",
      "Skipping v_proj because it is already a LinearEpsilon\n",
      "Skipping o_proj because it is already a LinearEpsilon\n",
      "INIT_MOD_MAP Replacing softmax with SoftmaxDT\n",
      "Skipping gate_proj because it is already a LinearEpsilon\n",
      "Skipping up_proj because it is already a LinearEpsilon\n",
      "Skipping down_proj because it is already a LinearEpsilon\n",
      "Skipping score because it is already a LinearEpsilon\n",
      "+-----------------+------------+------------+------------------+\n",
      "| Parent Module   | Function   | Replaced   | LRP compatible   |\n",
      "+=================+============+============+==================+\n",
      "+-----------------+------------+------------+------------------+\n",
      "Model after AttnLRP rules: LlamaForSequenceClassification(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048, padding_idx=2)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): LinearEpsilon(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): LinearEpsilon(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): LinearEpsilon(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): LinearEpsilon(in_features=2048, out_features=2048, bias=False)\n",
      "          (softmax): SoftmaxDT(dim=-1)\n",
      "          (attn_value_matmul): UniformEpsilonRule(\n",
      "            (module): AttentionValueMatmul()\n",
      "          )\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): LinearEpsilon(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): LinearEpsilon(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): LinearEpsilon(in_features=5632, out_features=2048, bias=False)\n",
      "          (silu): IdentityRule(\n",
      "            (module): SiLU()\n",
      "          )\n",
      "          (proj_silu_mul): UniformRule(\n",
      "            (module): ProjSiluMultiplication()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): RMSNormIdentity()\n",
      "        (post_attention_layernorm): RMSNormIdentity()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNormIdentity()\n",
      "  )\n",
      "  (score): LinearEpsilon(in_features=2048, out_features=17, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"load pretrained llama\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.llama.modeling_llama import LlamaForSequenceClassification\n",
    "from lxt.models.llama import (\n",
    "    # LlamaForSequenceClassification,\n",
    "    attnlrp\n",
    ")\n",
    "\n",
    "#model_name = f\"{CHECKPOINT_PATH}/meta-llama/Meta-Llama-3-8B-ft-zo_up/checkpoint-2200/\"\n",
    "#model_name = f\"{CHECKPOINT_PATH}/meta-llama/Meta-Llama-3-8B-fullft-zo_up/checkpoint-1500/\"\n",
    "model_name = f\"{CHECKPOINT_PATH}/TinyLlama/TinyLlama_v1.1/checkpoint-100/\"\n",
    "#model_name = f\"{CHECKPOINT_PATH}/TinyLlama/TinyLlama_v1.1-ft-zo_up/checkpoint-160/\"\n",
    "#model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model = LlamaForSequenceClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=17,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     token=HF_TOKEN\n",
    "# )\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# print(\"Model before AttnLRP rules:\", model)\n",
    "\n",
    "# # apply AttnLRP rules\n",
    "# print(\"=\" * 40, \"\\n\")\n",
    "# print(\"Model after AttnLRP rules:\", attnlrp.register(model))\n",
    "\n",
    "\n",
    "# Use AttnLRP LlamaForSequenceClassification model for comparison\n",
    "from lxt.models.llama import LlamaForSequenceClassification as LlamaForSequenceClassificationAttnLRP\n",
    "\n",
    "model_attnlrp = LlamaForSequenceClassificationAttnLRP.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=17,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "model_attnlrp.eval()\n",
    "print(\"=\" * 40, \"\\n\")\n",
    "print(\"AttnLRP llama rules:\", model_attnlrp)\n",
    "\n",
    "# from peft import PeftModel\n",
    "\n",
    "# peft_model = PeftModel.from_pretrained(model_attnlrp, model_id=model_name)\n",
    "# model_attnlrp = peft_model.merge_and_unload()\n",
    "\n",
    "print(\"=\" * 40, \"\\n\")\n",
    "print(\"Model after AttnLRP rules:\", attnlrp.register(model_attnlrp, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"This is about clean energy, and also affordable energy.\"\n",
    "#sample_sentence = \"Is this about clean energy?\"\n",
    "#sample_sentence = \"In terms of regional shares in the OECD area, OECD Europe’s share of consumption is slightly higher than the region’s share of extraction, while the inverse if tme for the OECD America region. The OECD Asia-Oceania region’s share of consumption is the same as its share of extraction. Average income plays a particularly important role. Most of these countries experienced a strong upswing in material extraction starting the early 2000s, although China’s surge began much earlier. By the early 1990s China had overtaken the United States as the world’s largest extractor of material resources.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [3, 26, 17, 94, 3029, 477, 585, 8, 67, 4920, 477, 48, 4]\n",
      "Decoded sentence: this is about clean energy, and also affordable energy.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentence\n",
    "tokenized_output = tokenizer(sample_sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "# Extract token IDs\n",
    "token_ids = tokenized_output['input_ids'][0].tolist()\n",
    "\n",
    "# Decode token IDs to readable string\n",
    "decoded_sentence = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Decoded sentence:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape torch.Size([1, 17])\n",
      "torch.Size([1, 13, 2048])\n",
      "torch.Size([1, 13, 2048])\n",
      "tensor(3.656250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward0>) 6\n",
      "['[CLS]', 'this', 'is', 'about', 'clean', 'energy', ',', 'and', 'also', 'affordable', 'energy', '.', '[SEP]']\n",
      "this is about clean energy, and also affordable energy.\n",
      "tensor([     0.000000,      0.000000,      0.000000,      0.000000,\n",
      "             0.110783,     -0.717498,     -1.000000,     -0.000000,\n",
      "             0.000000,      0.364351,     -0.303981,      0.000000,\n",
      "             0.000000])\n"
     ]
    }
   ],
   "source": [
    "from lxt.utils import clean_tokens\n",
    "import lxt.functional as lf\n",
    "\n",
    "model = model_attnlrp\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    sample_sentence,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,\n",
    ").input_ids.to(model.device)\n",
    "input_embeds = model.get_input_embeddings()(input_ids)\n",
    "\n",
    "output_logits = model(inputs_embeds=input_embeds.requires_grad_(), use_cache=False).logits\n",
    "print(\"output shape\", output_logits.shape)\n",
    "\n",
    "max_logits, max_indices = torch.max(output_logits[0, :], dim=-1)\n",
    "\n",
    "print(input_embeds.shape)\n",
    "max_logits.backward(max_logits)\n",
    "print(input_embeds.grad.shape)\n",
    "relevance = input_embeds.grad.float().sum(-1).cpu()[0]\n",
    "\n",
    "# normalize relevance between [-1, 1] for plotting\n",
    "relevance = relevance / relevance.abs().max()\n",
    "\n",
    "# remove '_' characters from token strings\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "tokens = clean_tokens(tokens)\n",
    "\n",
    "print(max_logits, max_indices.item())\n",
    "print(tokens)\n",
    "print(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "print(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNormEpsilon((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNormEpsilon((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNormEpsilon((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): ModulesToSaveWrapper(\n",
      "    (original_module): Linear(in_features=768, out_features=17, bias=True)\n",
      "    (modules_to_save): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=17, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "INIT_MOD_MAP Replacing query with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LoraLinearLRP\n",
      "INIT_MOD_MAP Replacing base_layer with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing original_module with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing default with LinearEpsilon\n",
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNormEpsilon((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): lora.Linear(\n",
      "                (base_layer): LinearEpsilon(in_features=768, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): LinearEpsilon(in_features=768, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): LinearEpsilon(in_features=16, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (key): LinearEpsilon(in_features=768, out_features=768, bias=True)\n",
      "              (value): lora.Linear(\n",
      "                (base_layer): LinearEpsilon(in_features=768, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): LinearEpsilon(in_features=768, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): LinearEpsilon(in_features=16, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): LinearEpsilon(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNormEpsilon((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): LinearEpsilon(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): IdentityRule(\n",
      "              (module): GELUActivation()\n",
      "            )\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): LinearEpsilon(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNormEpsilon((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): LinearEpsilon(in_features=768, out_features=768, bias=True)\n",
      "      (activation): IdentityRule(\n",
      "        (module): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): ModulesToSaveWrapper(\n",
      "    (original_module): LinearEpsilon(in_features=768, out_features=17, bias=True)\n",
      "    (modules_to_save): ModuleDict(\n",
      "      (default): LinearEpsilon(in_features=768, out_features=17, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 17])\n",
      "The label of the sequence is:  7\n",
      "['▁[CLS]', '▁is', '▁this', '▁about', '▁clean', '▁energy', '▁?', '▁[SEP]']\n",
      "tensor([-0.053514, -0.057133, -0.113126, -0.027830,  0.160663,  1.000000,\n",
      "        -0.109830,  0.062150])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Requires `pip install -e ./lxt`\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from lxt.models.bert import attnlrp, BertForSequenceClassification\n",
    "from lxt.utils import pdf_heatmap, clean_tokens\n",
    "\n",
    "def clean_wordpiece_split(tokens):\n",
    "        \"\"\" BERT-specific cleaning. Workaround not working perfect yet.\"\"\"\n",
    "        return [\"▁\" + word.replace(\"##\", \"\") for word in tokens]\n",
    "\n",
    "def seq_cls():\n",
    "    \"\"\"AttnLRP for BERT sequence classification task.\"\"\"\n",
    "    SCIBERT_MODEL = CHECKPOINT_PATH + \"/allenai/scibert_scivocab_uncased-ft-lora-zo_up/checkpoint-84/\"\n",
    "    #SCIBERT_MODEL = CHECKPOINT_PATH + \"/allenai/scibert_scivocab_uncased-ft-zo_up-lower/checkpoint-240/\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        SCIBERT_MODEL\n",
    "    )\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        SCIBERT_MODEL,\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    ).to(torch.device(\"cuda\"))\n",
    "    model.eval()\n",
    "    print(model)\n",
    "\n",
    "    # apply AttnLRP rules\n",
    "    attnlrp.register(model)\n",
    "    print(model)\n",
    "\n",
    "    input_ids = tokenizer(sample_sentence, return_tensors=\"pt\").input_ids.to(torch.device(\"cuda\"))\n",
    "    inputs_embeds = model.bert.get_input_embeddings()(input_ids)\n",
    "\n",
    "    logits = model(inputs_embeds=inputs_embeds.requires_grad_()).logits\n",
    "    print(logits.shape)\n",
    "\n",
    "    # We explain the sequence label: acceptable or unacceptable\n",
    "    max_logits, max_indices = torch.max(logits, dim=-1)\n",
    "\n",
    "    out = model.config.id2label[max_indices.item()]\n",
    "    print(\"The label of the sequence is: \", out)\n",
    "\n",
    "    max_logits.backward(max_logits)\n",
    "\n",
    "    relevance = inputs_embeds.grad.float().sum(-1).cpu()[0]\n",
    "    # normalize relevance between [-1, 1] for plotting\n",
    "    relevance = relevance / relevance.abs().max()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    tokens = clean_tokens(clean_wordpiece_split(tokens))\n",
    "\n",
    "    #pdf_heatmap(tokens, relevance, path=\"./heatmap_seq_cls.pdf\", backend=\"xelatex\")\n",
    "    print(tokens)\n",
    "    print(relevance)\n",
    "\n",
    "seq_cls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinBERT attnlrp on Sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.712738,  2.973304, -2.078524]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "INIT_MOD_MAP Replacing query with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing query with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing key with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing value with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing dense with LinearEpsilon\n",
      "INIT_MOD_MAP Replacing classifier with LinearEpsilon\n",
      "torch.Size([1, 3])\n",
      "The label of the sequence is:  Bullish\n",
      "['▁[CLS]', '▁citi', '▁gives', '▁big', '▁boost', '▁to', '▁deere', '▁pt', '▁[SEP]']\n",
      "tensor([-0.044550, -0.045770,  0.033312,  0.185180,  1.000000,  0.007016,\n",
      "        -0.002150,  0.330361, -0.556141])\n"
     ]
    }
   ],
   "source": [
    "finbert = BertForSequenceClassification.from_pretrained(\n",
    "    \"nickmuchi/finbert-tone-finetuned-fintwitter-classification\",\n",
    "    num_labels=3,\n",
    ").to(\"cuda\")\n",
    "finbert.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"nickmuchi/finbert-tone-finetuned-fintwitter-classification\"\n",
    ")\n",
    "\n",
    "sentence = \"Citi gives big boost to Deere PT\"\n",
    "print(finbert(input_ids=tokenizer(sentence, return_tensors=\"pt\").input_ids.to(\"cuda\")))\n",
    "attnlrp.register(finbert)\n",
    "\n",
    "input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids.to(torch.device(\"cuda\"))\n",
    "inputs_embeds = finbert.bert.get_input_embeddings()(input_ids)\n",
    "\n",
    "logits = finbert(inputs_embeds=inputs_embeds.requires_grad_()).logits\n",
    "print(logits.shape)\n",
    "\n",
    "# We explain the sequence label: acceptable or unacceptable\n",
    "max_logits, max_indices = torch.max(logits, dim=-1)\n",
    "\n",
    "out = finbert.config.id2label[max_indices.item()]\n",
    "print(\"The label of the sequence is: \", out)\n",
    "\n",
    "max_logits.backward(max_logits)\n",
    "\n",
    "relevance = inputs_embeds.grad.float().sum(-1).cpu()[0]\n",
    "# normalize relevance between [-1, 1] for plotting\n",
    "relevance = relevance / relevance.abs().max()\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "tokens = clean_tokens(clean_wordpiece_split(tokens))\n",
    "\n",
    "#pdf_heatmap(tokens, relevance, path=\"./heatmap_seq_cls.pdf\", backend=\"xelatex\")\n",
    "print(tokens)\n",
    "print(relevance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
