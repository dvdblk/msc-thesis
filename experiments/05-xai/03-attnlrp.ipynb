{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ipynb_util_tars.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=6, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"Is this about clean energy?\"\n",
    "#sample_sentence = \"In terms of regional shares in the OECD area, OECD Europe’s share of consumption is slightly higher than the region’s share of extraction, while the inverse if tme for the OECD America region. The OECD Asia-Oceania region’s share of consumption is the same as its share of extraction. Average income plays a particularly important role. Most of these countries experienced a strong upswing in material extraction starting the early 2000s, although China’s surge began much earlier. By the early 1990s China had overtaken the United States as the world’s largest extractor of material resources.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LinearEpsilon(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): LinearEpsilon(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): LinearEpsilon(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): LinearEpsilon(in_features=2048, out_features=2048, bias=False)\n",
       "          (softmax): SoftmaxDT(dim=-1)\n",
       "          (attn_value_matmul): UniformEpsilonRule(\n",
       "            (module): AttentionValueMatmul()\n",
       "          )\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LinearEpsilon(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): LinearEpsilon(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): LinearEpsilon(in_features=5632, out_features=2048, bias=False)\n",
       "          (silu): IdentityRule(\n",
       "            (module): SiLU()\n",
       "          )\n",
       "          (proj_silu_mul): UniformRule(\n",
       "            (module): ProjSiluMultiplication()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): RMSNormIdentity()\n",
       "        (post_attention_layernorm): RMSNormIdentity()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNormIdentity()\n",
       "  )\n",
       "  (score): LinearEpsilon(in_features=2048, out_features=17, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"load pretrained llama\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from lxt.models.llama import LlamaForSequenceClassification, attnlrp\n",
    "\n",
    "#model_name = f\"{CHECKPOINT_PATH}/meta-llama/Meta-Llama-3-8B-ft-zo_up/checkpoint-2200/\"\n",
    "model_name = f\"{CHECKPOINT_PATH}/TinyLlama/TinyLlama_v1.1/checkpoint-100/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=17,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# apply AttnLRP rules\n",
    "attnlrp.register(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1, 1317, 445, 1048, 5941, 5864, 29973]\n",
      "Decoded sentence: Is this about clean energy?\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentence\n",
    "tokenized_output = tokenizer(sample_sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "# Extract token IDs\n",
    "token_ids = tokenized_output['input_ids'][0].tolist()\n",
    "\n",
    "# Decode token IDs to readable string\n",
    "decoded_sentence = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Decoded sentence:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape torch.Size([1, 17])\n",
      "torch.Size([1, 7, 2048])\n",
      "torch.Size([1, 7, 2048])\n",
      "tensor(0.984375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward0>) 14\n",
      "Is this about clean energy?\n",
      "['<s>', '▁Is', '▁this', '▁about', '▁clean', '▁energy', '?']\n",
      "Is this about clean energy?\n",
      "tensor([ 0.135611, -0.075381,  0.294390,  0.168418,  0.485919,  1.000000,\n",
      "         0.426518])\n"
     ]
    }
   ],
   "source": [
    "from lxt.utils import clean_tokens\n",
    "import lxt.functional as lf\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    sample_sentence,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,\n",
    ").input_ids.to(model.device)\n",
    "input_embeds = model.get_input_embeddings()(input_ids)\n",
    "\n",
    "output_logits = model(inputs_embeds=input_embeds.requires_grad_(), use_cache=False).logits\n",
    "print(\"output shape\", output_logits.shape)\n",
    "\n",
    "max_logits, max_indices = torch.max(lf.softmax(output_logits[0, :], -1), dim=-1)\n",
    "\n",
    "print(input_embeds.shape)\n",
    "max_logits.backward(max_logits)\n",
    "print(input_embeds.grad.shape)\n",
    "relevance = input_embeds.grad.float().sum(-1).cpu()[0]\n",
    "\n",
    "# normalize relevance between [-1, 1] for plotting\n",
    "relevance = relevance / relevance.abs().max()\n",
    "\n",
    "# remove '_' characters from token strings\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "tokens = clean_tokens(tokens)\n",
    "\n",
    "print(max_logits, max_indices.item())\n",
    "print(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "print(tokens)\n",
    "print(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "print(relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output shape torch.Size([1, 17])\n",
    "torch.Size([1, 120, 4096])\n",
    "torch.Size([1, 120, 4096])\n",
    "tensor(1., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward0>) 3\n",
    "In terms of regional shares in the OECD area, OECD Europe’s share of consumption is slightly higher than the region’s share of extraction, while the inverse if tme for the OECD America region. The OECD Asia-Oceania region’s share of consumption is the same as its share of extraction. Average income plays a particularly important role. Most of these countries experienced a strong upswing in material extraction starting the early 2000s, although China’s surge began much earlier. By the early 1990s China had overtaken the United States as the world’s largest extractor of material resources.\n",
    "['<|begin\\\\_of\\\\_text|>', 'In', 'Ġterms', 'Ġof', 'Ġregional', 'Ġshares', 'Ġin', 'Ġthe', 'ĠOECD', 'Ġarea', ',', 'ĠOECD', 'ĠEurope', 'âĢĻs', 'Ġshare', 'Ġof', 'Ġconsumption', 'Ġis', 'Ġslightly', 'Ġhigher', 'Ġthan', 'Ġthe', 'Ġregion', 'âĢĻs', 'Ġshare', 'Ġof', 'Ġextraction', ',', 'Ġwhile', 'Ġthe', 'Ġinverse', 'Ġif', 'Ġt', 'me', 'Ġfor', 'Ġthe', 'ĠOECD', 'ĠAmerica', 'Ġregion', '.', 'ĠThe', 'ĠOECD', 'ĠAsia', '-O', 'ce', 'ania', 'Ġregion', 'âĢĻs', 'Ġshare', 'Ġof', 'Ġconsumption', 'Ġis', 'Ġthe', 'Ġsame', 'Ġas', 'Ġits', 'Ġshare', 'Ġof', 'Ġextraction', '.', 'ĠAverage', 'Ġincome', 'Ġplays', 'Ġa', 'Ġparticularly', 'Ġimportant', 'Ġrole', '.', 'ĠMost', 'Ġof', 'Ġthese', 'Ġcountries', 'Ġexperienced', 'Ġa', 'Ġstrong', 'Ġup', 'swing', 'Ġin', 'Ġmaterial', 'Ġextraction', 'Ġstarting', 'Ġthe', 'Ġearly', 'Ġ', '200', '0', 's', ',', 'Ġalthough', 'ĠChina', 'âĢĻs', 'Ġsurge', 'Ġbegan', 'Ġmuch', 'Ġearlier', '.', 'ĠBy', 'Ġthe', 'Ġearly', 'Ġ', '199', '0', 's', 'ĠChina', 'Ġhad', 'Ġovert', 'aken', 'Ġthe', 'ĠUnited', 'ĠStates', 'Ġas', 'Ġthe', 'Ġworld', 'âĢĻs', 'Ġlargest', 'Ġextractor', 'Ġof', 'Ġmaterial', 'Ġresources', '.']\n",
    "In terms of regional shares in the OECD area, OECD Europe’s share of consumption is slightly higher than the region’s share of extraction, while the inverse if tme for the OECD America region. The OECD Asia-Oceania region’s share of consumption is the same as its share of extraction. Average income plays a particularly important role. Most of these countries experienced a strong upswing in material extraction starting the early 2000s, although China’s surge began much earlier. By the early 1990s China had overtaken the United States as the world’s largest extractor of material resources.\n",
    "tensor([    -0.461020,      0.336821,     -1.000000,      0.403909,\n",
    "            -0.007544,     -0.067217,     -0.251409,      0.239639,\n",
    "             0.071959,     -0.001595,     -0.142529,      0.017504,\n",
    "            -0.304663,      0.253049,     -0.001901,      0.069398,\n",
    "             0.019843,     -0.104641,      0.175546,      0.983041,\n",
    "            -0.363505,     -0.021442,     -0.006289,     -0.031607,\n",
    "             0.102681,     -0.002945,     -0.018754,      0.015238,\n",
    "             0.003156,     -0.092878,     -0.015317,     -0.051677,\n",
    "            -0.043322,      0.084800,      0.076746,      0.053048,\n",
    "            -0.024949,     -0.010645,     -0.003498,      0.009203,\n",
    "            -0.017288,      0.021517,      0.013976,      0.061074,\n",
    "            -0.008544,     -0.006896,     -0.001114,      0.002361,\n",
    "             0.001473,      0.002007,      0.006377,      0.002052,\n",
    "             0.000777,      0.002239,     -0.009535,      0.018566,\n",
    "             0.002728,      0.042582,     -0.000808,     -0.012743,\n",
    "            -0.436750,     -0.003314,      0.011133,     -0.000627,\n",
    "             0.000611,      0.001883,      0.000528,      0.001089,\n",
    "             0.000024,      0.000144,     -0.000012,     -0.000109,\n",
    "             0.000285,      0.000057,     -0.000205,      0.000113,\n",
    "            -0.000153,     -0.000203,      0.000350,     -0.000049,\n",
    "            -0.000009,     -0.000140,     -0.000142,      0.000014,\n",
    "             0.001060,     -0.000014,      0.000004,     -0.000011,\n",
    "            -0.000005,     -0.000003,     -0.000001,      0.000014,\n",
    "             0.000054,      0.000171,      0.000005,     -0.000007,\n",
    "            -0.000050,     -0.000000,      0.000000,      0.000000,\n",
    "             0.000000,      0.000000,      0.000000,     -0.000000,\n",
    "             0.000000,     -0.000000,     -0.000000,      0.000000,\n",
    "             0.000000,      0.000000,      0.000000,      0.000000,\n",
    "            -0.000000,     -0.000000,     -0.000000,      0.000000,\n",
    "            -0.000000,      0.000000,      0.000000,     -0.000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "The label of the sequence is:  LABEL_0\n",
      "['▁[CLS]', '▁i', '▁are', '▁a', '▁student', '▁.', '▁[SEP]']\n",
      "tensor([-0.164881, -0.258037,  0.871932, -0.442716, -0.275364, -0.070098,\n",
      "        -1.000000])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Requires `pip install -e ./lxt`\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from lxt.models.bert import attnlrp, BertForSequenceClassification\n",
    "from lxt.utils import pdf_heatmap, clean_tokens\n",
    "\n",
    "def clean_wordpiece_split(tokens):\n",
    "        \"\"\" BERT-specific cleaning. Workaround not working perfect yet.\"\"\"\n",
    "        return [\"▁\" + word.replace(\"##\", \"\") for word in tokens]\n",
    "\n",
    "def seq_cls():\n",
    "    \"\"\"AttnLRP for BERT sequence classification task.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-CoLA\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-CoLA\").to(torch.device(\"cuda\"))\n",
    "    model.eval()\n",
    "\n",
    "    # apply AttnLRP rules\n",
    "    attnlrp.register(model)\n",
    "\n",
    "    inputs = \"I are a student.\"\n",
    "\n",
    "    input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids.to(torch.device(\"cuda\"))\n",
    "    inputs_embeds = model.bert.get_input_embeddings()(input_ids)\n",
    "\n",
    "    logits = model(inputs_embeds=inputs_embeds.requires_grad_()).logits\n",
    "    print(logits.shape)\n",
    "\n",
    "    # We explain the sequence label: acceptable or unacceptable\n",
    "    max_logits, max_indices = torch.max(logits, dim=-1)\n",
    "\n",
    "    out = model.config.id2label[max_indices.item()]\n",
    "    print(\"The label of the sequence is: \", out)\n",
    "\n",
    "    max_logits.backward(max_logits)\n",
    "\n",
    "    relevance = inputs_embeds.grad.float().sum(-1).cpu()[0]\n",
    "    # normalize relevance between [-1, 1] for plotting\n",
    "    relevance = relevance / relevance.abs().max()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    tokens = clean_tokens(clean_wordpiece_split(tokens))\n",
    "\n",
    "    #pdf_heatmap(tokens, relevance, path=\"./heatmap_seq_cls.pdf\", backend=\"xelatex\")\n",
    "    print(tokens)\n",
    "    print(relevance)\n",
    "\n",
    "seq_cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
